{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 自然言語処理 100本ノック"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 自然言語とは\n",
    "英語や日本語の様に人間が自然発生的に使用してきた言語の事を自然言語処理といいます。<br>\n",
    "これに対し、プログラミング言語のような人工言語は形式言語と呼んで区別されます。<br>\n",
    "自然言語 <-> 形式言語"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 自然言語処理とは"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "自然言語を計算機(コンピューター)で処理する事を指します。\n",
    "計算機で処理を行う場合、言葉を計算機が計算しやすい様に、数値に変換する必要があります。\n",
    "画像や音声の場合は、ピクセル数や音の高さ等で表現(数値化)することができます。\n",
    "それに対して、自然言語の場合は、目的によって最適な表現方法(数値化)が異なる為、数値化が難しいとされています。\n",
    "簡単な方法としては、単語をあるなし(0, 1)で数値化する方法があります。\n",
    "これでは、単語ごとの意味が加味されていませんので、現在は、単語や文章の特徴を数値化して計算機で計算を行うことが主流となっています。\n",
    "具体的な特徴として、文章に出現する単語の頻出頻度や近年は単語同士の関連性が使用されており、また、特徴量の算出方法(アルゴリズム)も複数存在し、目的によって最適な方法が複数取られています。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 自然言語処理の活用"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "自然言語処理と機械学習を組み合わせることで、一例として以下のような事(用語としてはタスクといいます)がこなせます。\n",
    "\n",
    "- 文章分類\n",
    "- 自動要約\n",
    "- 機械翻訳\n",
    "- 文章生成\n",
    "- 著者推定\n",
    "- 自動ダグ生成"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 自然言語処理の基本的な流れ\n",
    "自然言語を計算機で処理する際の基本的な流れは以下のようなものです。\n",
    "ここでは、日本語に対しての処理の流れを説明します。\n",
    "\n",
    "形態素解析\n",
    "形態素解析器を使用して文を最小単位に分割する。また、各単語を品詞区分に紐付ける。\n",
    "今日はいい天気 -> \"今日\", \"は\", \"いい\", \"天気\"\n",
    "\"今日\": 名詞, \"は\": 助詞, \"いい\": 形容詞, \"天気\":名詞\n",
    "構文解析\n",
    "形態素解析を行った単語同士の関係性を解析する\n",
    "詳細(自然言語（日本語）処理)\n",
    "意味解析\n",
    "構文解析をした結果、複数の候補がある場合、意味が伝わる構文を選択する\n",
    "詳細(自然言語（日本語）処理)\n",
    "文脈解析\n",
    "複数の文章にまたがる構文解析+意味解析を行う\n",
    "近年は、(特に英語ですが)形態素解析を行い、その単語をベクトル化(数値化)するだけで様々なタスクでよい精度が出ています。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 以下スライドによる解説\n",
    "\n",
    "![](utils/自然言語処理/Slide1.jpg)\n",
    "![](utils/自然言語処理/Slide4.jpg)\n",
    "![](utils/自然言語処理/Slide5.jpg)\n",
    "![](utils/自然言語処理/Slide6.jpg)\n",
    "![](utils/自然言語処理/Slide7.jpg)\n",
    "![](utils/自然言語処理/Slide8.jpg)\n",
    "![](utils/自然言語処理/Slide9.jpg)\n",
    "![](utils/自然言語処理/Slide10.jpg)\n",
    "![](utils/自然言語処理/Slide11.jpg)\n",
    "![](utils/自然言語処理/Slide12.jpg)\n",
    "![](utils/自然言語処理/Slide13.jpg)\n",
    "![](utils/自然言語処理/Slide14.jpg)\n",
    "![](utils/自然言語処理/Slide15.jpg)\n",
    "![](utils/自然言語処理/Slide16.jpg)\n",
    "![](utils/自然言語処理/Slide17.jpg)\n",
    "![](utils/自然言語処理/Slide18.jpg)\n",
    "![](utils/自然言語処理/Slide28.jpg)\n",
    "![](utils/自然言語処理/Slide29.jpg)\n",
    "![](utils/自然言語処理/Slide30.jpg)\n",
    "![](utils/自然言語処理/Slide31.jpg)\n",
    "![](utils/自然言語処理/Slide32.jpg)\n",
    "![](utils/自然言語処理/Slide33.jpg)\n",
    "![](utils/自然言語処理/Slide34.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "本項では自然言語処理の基礎知識と応用について体験してもらいます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Python正規表現の基本"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pythonではパッケージreを使って正規表現の実装します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 関数を直接使用"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "re.matchやre.subなどの関数を使います。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<re.Match object; span=(0, 3), match='Hel'>\n",
      "Hel\n"
     ]
    }
   ],
   "source": [
    "# 第1引数が正規表現パターン(検索語句)、第2引数が検索対象\n",
    "result = re.match('Hel', 'Hellow python')\n",
    "\n",
    "print(result)\n",
    "# <_sre.SRE_Match object; span=(0, 3), match='Hel'>\n",
    "\n",
    "print(result.group())\n",
    "# Hel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 コンパイルして使用"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "正規表現パターンをコンパイルした後にmatchやsubなどの関数を使用します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<re.Match object; span=(0, 3), match='Hel'>\n",
      "Hel\n"
     ]
    }
   ],
   "source": [
    "# あらかじめ正規表現パターンをコンパイル\n",
    "regex = re.compile('Hel')\n",
    "\n",
    "result = regex.match('Hellow python')\n",
    "\n",
    "print(result)\n",
    "# <_sre.SRE_Match object; span=(0, 3), match='Hel'>\n",
    "\n",
    "print(result.group())\n",
    "# Hel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2種類の使い分け\n",
    "複数の正規表現パターンを何度も使う場合はコンパイル方式を使います。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">re.compile() を使い、結果の正規表現オブジェクトを保存して再利用するほうが、一つのプログラムでその表現を何回も使うときに効率的です。\n",
    "re.compile() やモジュールレベルのマッチング関数に渡された最新のパターンはコンパイル済みのものがキャッシュされるので、一度に正規表現を少ししか使わないプログラムでは正規表現をコンパイルする必要はありません。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 正規表現パターン(検索語句)の定義"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 raw文字列でエスケープシーケンス無効"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "raw文字列(raw string)は正規表現固有のトピックではありませんが、使うことでエスケープシーケンスを無効にできます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a\tb\n",
      "A\tB\n",
      "a\\tb\\nA\\tB\n"
     ]
    }
   ],
   "source": [
    "print('a\\tb\\nA\\tB')\n",
    "print(r'a\\tb\\nA\\tB')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "正規表現パターン内でバックスラッシュなどに対してエスケープシーケンスを書きたくないのでraw文字列を使います"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<re.Match object; span=(0, 1), match='3'>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = re.match(r'\\d', '329')\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 トリプルクォートとre.VERBOSEで改行・コメント・空白無視"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'''トリプルクォート(\"\"\"でも可能)で囲むことにより、正規表現パターン中に改行を使うことができます(改行がなくても問題なし)。\n",
    "re.VERBOSEを渡すことで、空白やコメントを正規表現パターンから除外できます。\n",
    "リプルクォートとre.VERBOSEで非常に読みやすくなります\n",
    "以下のような正規表現パターンを記述すると見やすいです。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = re.compile(r'''\\d +  # the integral part\n",
    "                   \\.    # the decimal point\n",
    "                   \\d *  # some fractional digits''', re.VERBOSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "トリプルクォートに関しては、記事「Pythonで文字列生成（引用符、strコンストラクタ）」に詳しくかかれています。\n",
    "\n",
    "ちなみにcompileのパラメータflagsで複数のコンパイルフラグを使いたい場合は、単純に+(加算)してやればOKです。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "re.compile(r'\\d', re.MULTILINE|re.UNICODE|re.VERBOSE)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = re.compile(r'''\\d''', re.VERBOSE+re.MULTILINE)\n",
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 OR条件（文字単体）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "文字列の一部で、いずれかにヒットするみたいなOR条件的なパターンを使うには[]の括弧を利用します（文字の集合といった具合に呼ばれます。英語だとcharacter classなど）。\n",
    "\n",
    "例えば、「特定の文字部分が猫もしくは犬のどちらかにヒットする」といった条件のパターンは以下のように設定できます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<re.Match object; span=(0, 8), match='吾輩は猫である。'>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.search(\n",
    "    pattern=r'吾輩は[猫犬]である。',\n",
    "    string='吾輩は猫である。名前はまだ無い。')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<re.Match object; span=(0, 8), match='吾輩は犬である。'>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.search(\n",
    "    pattern=r'吾輩は[猫犬]である。',\n",
    "    string='吾輩は犬である。名前はまだ無い。')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "re.search(\n",
    "    pattern=r'吾輩は[猫犬]である。',\n",
    "    string='吾輩は兎である。名前はまだ無い。')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<re.Match object; span=(0, 8), match='吾輩は兎である。'>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.search(\n",
    "    pattern=r'吾輩は[猫犬兎]である。',\n",
    "    string='吾輩は兎である。名前はまだ無い。')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 OR条件（文字列）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "先ほどは[]の括弧を使って文字単体のOR条件を試しました。文字単体ではなく文字列のOR条件を設定したい場合は条件の範囲を()の括弧で囲み、文字列間に|を挟みます（英語だとAlternation）。\n",
    "\n",
    "例えば、猫であると犬だよという文字列でOR条件を設定したい場合には以下のような書き方になります。猫犬両方の文字列でヒットします。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<re.Match object; span=(0, 8), match='吾輩は猫である。'>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.search(\n",
    "    pattern=r'吾輩は(猫である|犬だよ)。',\n",
    "    string='吾輩は猫である。名前はまだ無い。')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<re.Match object; span=(0, 7), match='吾輩は犬だよ。'>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "re.search(\n",
    "    pattern=r'吾輩は(猫である|犬だよ)。',\n",
    "    string='吾輩は犬だよ。名前はまだ無い。')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<re.Match object; span=(0, 7), match='吾輩は兎です。'>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.search(\n",
    "    pattern=r'吾輩は(猫である|犬だよ|兎です)。',\n",
    "    string='吾輩は兎です。名前はまだ無い。')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6 特殊文字\n",
    "| 文字 | 説明　| 備考 | 例 | マッチする| マッチしない | \n",
    "|------|-------|------|-----|---------|----------|\n",
    "|\\d\t|数字\t|[0-9]と同じ| | | \n",
    "| \\D\t|数字以外\t|[^0-9]と同じ|||\n",
    "|\\s |\t空白文字\t|[\\t\\n\\r\\f\\v]と同じ|||\t\t\t\n",
    "|\\S\t|空白文字以外\t|[^\\t\\n\\r\\f\\v]と同じ|||\t\t\t\n",
    "|\\w\t|英数文字と下線\t|[a-zA-Z0-9_]と同じ\t\t\t\n",
    "|\\W\t|英数文字以外\t|[\\a-zA-Z0-9_]と同じ\t\t\t\n",
    "|\\A\t|文字列の先頭\t|^と類似\t\t\t\n",
    "|\\Z\t|文字列の末尾\t|$と類似\t\t\t\n",
    "|\\b\t|単語の境界(スペース)\t\t\t\t\n",
    "|.\t|任意の一文字\t|-|\t1.3|\t123, 133|\t1223|\n",
    "|^\t|文字列の先頭\t|-|\t^123|\t1234|\t0123|\n",
    "|$\t|文字列の末尾\t|-|\t123$|\t0123|\t1234|\n",
    "|*\t|０回以上の繰り返し\t|-|\t12*|\t1, 12, 122\t|11, 22\n",
    "|+\t|１回以上の繰り返し\t|-|\t12+\t|12, 122\t|1, 11, 22\n",
    "|?\t|０回または１回\t|-|\t12? |1, 12|\t122|\n",
    "|{m}\t|m回の繰り返し|\t-|\t1{3}|\t111\t|11, 1111|\n",
    "|{m,n}\t|m〜n回の繰り返し\t|-\t|1{2, 3}\t|11, 111\t|1, 1111|\n",
    "|[]\t|集合\t|[^5]とすると5以外\t|[1-3]\t|1, 2, 3\t|4, 5|\n",
    "| &#124; |\t和集合(or)\t|-\t|1&#124;2\t|1, 2\t|3|\n",
    "|()\t|グループ化\t|-|\t(12)+|\t|12, 1212\t|1, 123|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. マッチ関数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以下の関数をよく使います。\n",
    "\n",
    "|関数\t|目的|\n",
    "|----|-----|\n",
    "|match\t|文字列の先頭で正規表現とマッチするか判定|\n",
    "|search\t|正規表現がどこにマッチするか検索|\n",
    "|findall|マッチする部分文字列を全て探しリストとして返します|\n",
    "|sub\t|文字列置換|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 matchとsearch\n",
    "文字列の先頭でのみのマッチするのがre.matchで文字列中の位置にかかわらずマッチするのがre.search。詳しくは公式の「search() vs. match()」を参照。両者とも最初のパターンのみを返します(2回目以降にマッチしたものを返さない)。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "re.match(\"c\", \"abcdef\")    # 先頭が\"c\"でないのでマッチしない"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<re.Match object; span=(2, 3), match='c'>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.search(\"c\", \"abcdef\")   # マッチする"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "結果はgroupに入っています。group(0)に結果がすべて入っていて、グループ化した検索結果は連番で1から入っています。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    " m = re.match(r\"(\\w+) (\\w+)\", \"Isaac Newton, physicist\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Isaac Newton'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " m.group(0)       # The entire match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Isaac'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.group(1)       # The first parenthesized subgroup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Newton'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.group(2)       # The second parenthesized subgroup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Isaac', 'Newton')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " m.group(1, 2)    # Multiple arguments give us a tuple."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 findall \n",
    "パターンにマッチした全ての文字列をリスト形式で返すのがfindall。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['carefully', 'quickly']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"He was carefully disguised but captured quickly by police.\"\n",
    "re.findall(r\"\\w+ly\", text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "()を使ってキャプチャ対象を指定できますが、複数指定した場合は以下のようになります。グループごとにタプルで返ってきます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('1st', '2nd'), ('1st', '2nd')]\n"
     ]
    }
   ],
   "source": [
    "print(re.findall(r'''(1st)(2nd)''', '1st2nd1st2nd'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 sub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "文字置換をします。引数の順に1. 正規表現パターン、2. 置換後の文字列、3. 置換対象文字列です。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'置換後 対象外 置換後'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " re.sub(r'置換前', '置換後', '置換前 対象外 置換前')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.　形態素解析"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 形態素解析とは "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "形態素解析とは、コンピュータを利用して機械的な方法で文を形態素（言葉で意味を持つ最小の単位）に区切る技術のことを言います。\n",
    "\n",
    "我々の身近な言語としては、日本語、英語があります。\n",
    "\n",
    "この２つの言語は、当然ですが、使用する言葉と文法が異なります。\n",
    "\n",
    "例えば、日本語で「これはペンです。」という文は、英語だと「This is a pen.」です。\n",
    "\n",
    "英語は、”This”, “is”, “a”, “pen”という風に、単語と単語の間に空白が存在しますが、日本語は単語と単語の間に空白が存在しません。\n",
    "\n",
    "このように、単語と単語の間に空白を入れる書き方を分かち書きといいます。\n",
    "\n",
    "そして、分かち書きされていない日本語の文章を機械に理解させるためには、単語分割を行うとともに品詞を明確にする必要があるのです。\n",
    "\n",
    "なぜなら文章は、決められた文法法則に従い単語に付与されている品詞を並べて構成されているからです。\n",
    "\n",
    "機械に対して、単語の意味、品詞、文法を理解させなければ意味を解釈させることもできなければ、文を作成させることもできません。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 形態素解析ツール"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "日本語の主な形態素解析ツールには以下のようなものがあります。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "・MeCab\n",
    "・ChaSen\n",
    "・JUMAN\n",
    "・Janome"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 形態素解析の実装 | MeCab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: mecab-python3 in /opt/conda/lib/python3.10/site-packages (1.0.5)\n"
     ]
    }
   ],
   "source": [
    "!pip install mecab-python3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以下のような出力が得られたら成功です。\n",
    "\n",
    "\n",
    "Collecting mecab-python3\n",
    "  Downloading https://files.pythonhosted.org/packages/c1/72/20f8f60b858556fdff6c0376b480c230e594621fff8be780603ac9c47f6a/mecab_python3-1.0.3-cp37-cp37m-manylinux1_x86_64.whl (487kB)\n",
    "     |████████████████████████████████| 491kB 8.6MB/s \n",
    "Installing collected packages: mecab-python3\n",
    "Successfully installed mecab-python3-1.0.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "次に、辞書データとしてUniDicをインストールします。\n",
    "UniDicは国立国語研究所が開発している辞書です。\n",
    "以下を入力し実行してください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting unidic\n",
      "  Downloading unidic-1.1.0.tar.gz (7.7 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: requests<3.0.0,>=2.22.0 in /opt/conda/lib/python3.10/site-packages (from unidic) (2.28.1)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.41.1 in /opt/conda/lib/python3.10/site-packages (from unidic) (4.64.1)\n",
      "Collecting wasabi<1.0.0,>=0.6.0\n",
      "  Downloading wasabi-0.10.1-py3-none-any.whl (26 kB)\n",
      "Collecting plac<2.0.0,>=1.1.3\n",
      "  Downloading plac-1.3.5-py2.py3-none-any.whl (22 kB)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0,>=2.22.0->unidic) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0,>=2.22.0->unidic) (1.26.11)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0,>=2.22.0->unidic) (2.1.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0,>=2.22.0->unidic) (2022.9.24)\n",
      "Building wheels for collected packages: unidic\n",
      "  Building wheel for unidic (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for unidic: filename=unidic-1.1.0-py3-none-any.whl size=7409 sha256=86be9bf7d71ad47451828cb300c4768432eba54afd7a57dd3d11e7bcb54f03a8\n",
      "  Stored in directory: /home/jovyan/.cache/pip/wheels/7a/72/72/1f3d654c345ea69d5d51b531c90daf7ba14cc555eaf2c64ab0\n",
      "Successfully built unidic\n",
      "Installing collected packages: wasabi, plac, unidic\n",
      "Successfully installed plac-1.3.5 unidic-1.1.0 wasabi-0.10.1\n"
     ]
    }
   ],
   "source": [
    "!pip install unidic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以下のような出力が得られたら成功です。\n",
    "Collecting unidic\n",
    "  Downloading https://files.pythonhosted.org/packages/86/04/c18832fd9959a78fc60eeaa9e7fb37ef31a250e8645cc2897eb1f07939ee/unidic-1.0.3.tar.gz\n",
    "Requirement already satisfied: requests<3.0.0,>=2.22.0 in /usr/local/lib/python3.7/dist-packages (from unidic) (2.23.0)\n",
    "Requirement already satisfied: tqdm<5.0.0,>=4.41.1 in /usr/local/lib/python3.7/dist-packages (from unidic) (4.41.1)\n",
    "Requirement already satisfied: wasabi<1.0.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from unidic) (0.8.2)\n",
    "Requirement already satisfied: plac<2.0.0,>=1.1.3 in /usr/local/lib/python3.7/dist-packages (from unidic) (1.1.3)\n",
    "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.22.0->unidic) (1.24.3)\n",
    "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.22.0->unidic) (2020.12.5)\n",
    "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.22.0->unidic) (2.10)\n",
    "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.22.0->unidic) (3.0.4)\n",
    "Building wheels for collected packages: unidic\n",
    "  Building wheel for unidic (setup.py) ... done\n",
    "  Created wheel for unidic: filename=unidic-1.0.3-cp37-none-any.whl size=5497 sha256=78ab4afc1982544644d7782dac00a36b55b71aceeb11cb7d0d692f01bd995f10\n",
    "  Stored in directory: /root/.cache/pip/wheels/d3/26/e2/fb76c79fd14391eb994eab021c9129c24814125298e1e5b96a\n",
    "Successfully built unidic\n",
    "Installing collected packages: unidic\n",
    "Successfully installed unidic-1.0.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "更に、辞書データもダウンロードします。\n",
    "\n",
    "以下のコードを実行しましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "download url: https://cotonoha-dic.s3-ap-northeast-1.amazonaws.com/unidic-3.1.0.zip\n",
      "Dictionary version: 3.1.0+2021-08-31\n",
      "Downloading UniDic v3.1.0+2021-08-31...\n",
      "unidic-3.1.0.zip:  16%|███▎                 | 82.3M/526M [00:41<03:22, 2.19MB/s]"
     ]
    }
   ],
   "source": [
    "!python -m unidic download"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以下のような出力が得られたら成功です。\n",
    "download url: https://cotonoha-dic.s3-ap-northeast-1.amazonaws.com/unidic.zip\n",
    "Dictionary version: 2.3.0+2020-10-08\n",
    "Downloading UniDic v2.3.0+2020-10-08...\n",
    "unidic.zip: 100% 608M/608M [00:22<00:00, 27.3MB/s]\n",
    "Finished download.\n",
    "Downloaded UniDic v2.3.0+2020-10-08 to /usr/local/lib/python3.7/dist-packag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 「私は形態素解析を学んでいます」という文章を解析します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import MeCab\n",
    "import unidic\n",
    "mecab = MeCab.Tagger()\n",
    "print(mecab.parse(\"私は形態素解析を学んでいます。\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "出力された内容を確認しましょう。出力される形態素解析の結果は、左から順に以下の通りとなります。\n",
    "\n",
    "表層形（surface）（文章中で使用されている単語）\n",
    "品詞（part_of_speech）\n",
    "品詞細分類1〜3（part_of_speech）\n",
    "活用型（infl_type）\n",
    "活用形（infl_form）\n",
    "原形（base_form）（文章中で使用されている単語の原形）\n",
    "読み（reading）\n",
    "発音（phonetic）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5 MeCabを用いて分かち書きをしたい場合"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "分かち書きをしたい場合は、Tagger()オブジェクトの出力モードに('-Owakati')を指定すればOKです。\n",
    "\n",
    "('-Owakati')を指定することで、品詞などを付与せず、形態素ごとに区切りの空白を入れることができます。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import MeCab\n",
    "import unidic\n",
    "mecab = MeCab.Tagger('-Owakati')\n",
    "print(mecab.parse(\"私は形態素解析を学んでいます。\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tagger()オブジェクトの出力モードには、以下も指定できます。\n",
    "\n",
    "- Oyomi: 読みのみを出力\n",
    "- Ochasen: ChaSen互換形式\n",
    "- Odump: 全ての情報を出力\n",
    "\n",
    "実行したい処理に応じて使い分けましょう。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.6 形態素解析の実装 | Janome"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "まずは、janomeをインストールしましょう。Google Colaboratoryのセルに以下を入力し実行すればOKです。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install janome"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以下のような出力が得られれば成功です。\n",
    "Collecting janome\n",
    "  Downloading https://files.pythonhosted.org/packages/a8/63/98858cbead27df7536c7e300c169da0999e9704d02220dc6700b804eeff0/Janome-0.4.1-py2.py3-none-any.whl (19.7MB)\n",
    "     |████████████████████████████████| 19.7MB 50.0MB/s \n",
    "Installing collected packages: janome\n",
    "Successfully installed janome-0.4.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.7 janomeを使った形態素解析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from janome.tokenizer import Tokenizer\n",
    "\n",
    "t = Tokenizer()\n",
    "tokens = t.tokenize(\"私は形態素解析を学んでいます。\")\n",
    "for token in tokens:\n",
    "  print(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "出力された内容は、MeCabと同様に、左から順に以下の通りとなります。\n",
    "\n",
    "表層形（surface）（文章中で使用されている単語）\n",
    "品詞（part_of_speech）\n",
    "品詞細分類1〜3（part_of_speech）\n",
    "活用型（infl_type）\n",
    "活用形（infl_form）\n",
    "原形（base_form）（文章中で使用されている単語の原形）\n",
    "読み（reading）\n",
    "発音（phonetic）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.8 Janomeを用いて分かち書きをしたい場合"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Janomeで分かち書きをしたい場合は、tokenize()メソッドの引数にwakati=Trueを指定すると、分かち書きのみを行います。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from janome.tokenizer import Tokenizer\n",
    "\n",
    "t = Tokenizer()\n",
    "tokens = t.tokenize(\"私は形態素解析を学んでいます。\", wakati=True)\n",
    "for token in tokens:\n",
    "  print(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.9　テキストファイルを読み込んで形態素解析を行う"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'data/wagahaiwa_nekodearu.txt'\n",
    "sentences = []\n",
    "\n",
    "tagger = MeCab.Tagger(\"-Owakati\")\n",
    "morphs = []\n",
    "with open(filename, mode='r', encoding=\"ShiftJIS\") as f:\n",
    "  for line in f:  # 1行ずつ読込\n",
    "    if line != 'EOS\\n':  # 文末以外：形態素解析情報を辞書型に格納して形態素リストに追加\n",
    "      fields = line.split('\\t')\n",
    "      sentences.append(tagger.parse(fields[0]))\n",
    "sentences[:4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 スクレイピングで青空文庫からデータを取得してみる\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "今回はスクレイピングで青空文庫からデータを取得をしてみます。\n",
    "### Webスクレイピングとは\n",
    "Webスクレイピングとは、Webページから情報を取得することを指します。Pythonを用いることで、取得だけでなく、取得した情報をExcelやGoogleスプレッドシートなどに整理し、利用できるようにすることも可能です。\n",
    "WebスクレイピングをするにはPythonとWebの基礎知識が必要になりますが、決して難しいものではありません。原理を理解することで、自分で使いやすいシステムを構築することができます。\n",
    "\n",
    "- Pythonで使えるWebスクレイピングのライブラリ<br>\n",
    "    PythonにはWebスクレイピングに使えるライブラリが用意されています。今回は代表的な3つのライブラリについて紹介していきます。\n",
    "- BeautifulSoup<br>\n",
    "    HTMLやXMLからデータを引き出せるライブラリです。Pythonでクローラーを作成する際によく使用されるライブラリですが、BeautifulSoup単体ではスクレイピングはできないため、HTTP通信ができるモジュールやCSVにエクスポートする他のライブラリと組み合わせて使用します。\n",
    "- Scrapy<br>\n",
    "    クローラーを実装・運用するために必要となる機能を持つ、アプリケーション全体を実装するためのフレームワークです。Webスクレイピング用に設計されましたが、APIを使用したデータ抽出や汎用クローラーとして使用することも可能です。\n",
    "- Selenium<br>\n",
    "    Webブラウザの操作を自動化するフレームワークです。本来はWebアプリケーションのUIテストを自動化するために開発されましたが、ブラウザの操作をコードで記述して自動化できる利便性の高さからタスクやWebサイトのクローリングなどに転用されています。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Webスクレイピングの注意点\n",
    "\n",
    "Webスクレイピングは、情報を収集するためにWebサイトに頻繁にアクセスします。アクセスする頻度によっては、Webサイトが設置されているサーバに大きな負荷をかけ、他のユーザーがアクセスしにくくなったり、サーバがダウンしてしまう、いわゆるDOS攻撃（Denial-of-service attack）になってしまうケースもあります。\n",
    "\n",
    "for分やwhile文で情報を取得することは禁止します。\n",
    "\n",
    "Webスクレイピングをおこなう際には、DOS攻撃にならないよう注意し、アクセスする間隔や頻度を調整するように気を付けましょう。悪意の有無に関わらず、DOS攻撃はサーバ負荷が大きく、アクセス先のWebサイトが設置されているサーバが共有の場合、最悪のケースでは他のユーザーを保護するためサイトがサーバから削除されることもあります。\n",
    "\n",
    "\n",
    "あくまで収集する情報を提供してもらっているという意識を忘れず、相手に迷惑をかけないプログラミングを心がけましょう。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 PythonでWebスクレイピング\n",
    "#### urllib.requestを利用したWebスクレイピング\n",
    "PythonにはURLを扱うためのモジュールとして、いくつかのモジュールをまとめたurllibモジュールパッケージが標準で付属しています。今回はこの標準モジュールのうち、urllib.reguestモジュールを利用してWebスクレイピングをおこないます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib import request\n",
    "response = request.urlopen('https://www.aozora.gr.jp/index.html')\n",
    "content = response.read()\n",
    "response.close()\n",
    "html = content.decode()\n",
    "\n",
    "title = html.split('<title>')[1].split('</title')[0]\n",
    "print(title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "まずurllib.request.urlopen関数でURLをオープンします。この関数でURLをオープンすると、サーバからはhttp.clientモジュールで定義されているHTTPResponseクラスのオブジェクトが返送されます。\n",
    "続いてreadメソッドを使用してWebページの内容（ソースコード）を取得し、URLをクローズします。\n",
    "ここまでの操作で取得したページの内容はbytesオブジェクト（バイト列）になっているため、decodeメソッドで文字列（str）にデコードします。\n",
    "最後に、文字列として取得できたデータから、今回はタイトルタグを取得するため、文字列操作でタイトルタグを検索して取得し、出力します。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BeautifulSoupを使用したWebスクレイピング\n",
    "ここからは、BeautifulSoupを使用したWebスクレイピングのサンプルコードを見ていきましょう。BeautifulSoupを使用すると、標準のurllibモジュールを使用するよりも簡潔なコードでWebスクレイピングを構築できます。\n",
    "\n",
    "前項で解説したとおり、BeautifulSoupは単体ではHTTPへの通信機能を持たないため、別のライブラリやパッケージと組み合わせて使用します。今回は「requests」というリクエスト用のパッケージを読み込み、URLを渡すことでWebページを読み込みます。\n",
    "\n",
    "まずrequestsとBeautifulSoupのライブラリをインポートします。続いて今回取得したいWebサイトのURLをrequestsのgetメソッドで展開してコンテンツを取得します。\n",
    "\n",
    "取得したコンテンツをresponseに格納してBeautifulSoupに渡し、responseの内容を解析します。最後に解析した内容をfindメソッドで検索して、get_textでテキストを取得し、出力します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "response = requests.get('https://www.aozora.gr.jp/index.html')\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "title = soup.find('title').get_text()\n",
    "print(title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 小説のデータを抽出してみる"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "データの取得にはBeautiful Soup 4を使います。まずは夏目漱石の小説を幾つか取得するコードを完成させていきましょう。\n",
    "[リンク](https://www.aozora.gr.jp/index_pages/person148.html)\n",
    "\n",
    "![](./utils/natume_souseki.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "今回は「こころ」を取得してみようと思います。国語の授業で読んだことがある人もいると思います。<br>\n",
    "リンクをクリックすると、次のようなページが表示されます。<br>\n",
    "![](./utils/kokoro.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "取得可能なファイルの種類には3つありますが、ここでは［ファイル種別］が［XHTMLファイル］となっているファイルを取得することにします。<br>\n",
    "このリンク（773_14560.html）をクリックすると以下のように作品が表示されます。<br>\n",
    "![](./utils/こころ.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "urllib.requestモジュールが提供するurlopen関数を使って、このページの内容を取得して、これをBeautiful Soup 4に入力すれば、さまざまな操作が可能なオブジェクトが手に入ります。\n",
    "ここまでの処理を実際に行うのが、以下のコードです。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from urllib import request\n",
    "\n",
    "url = 'https://www.aozora.gr.jp/cards/000074/files/427_19793.html'\n",
    "response = request.urlopen(url)\n",
    "soup = BeautifulSoup(response)\n",
    "response.close()\n",
    "\n",
    "print(soup)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "文テキストだけではなく、さまざまなタグが含まれています。そこで、まずはdiv＞タグ（class属性が\"main_text\"）となっている部分だけを取り出しましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_text = soup.find('div', class_='main_text')\n",
    "print(main_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "これで小説の本文テキストだけが得られましたが、気になるのは山ほど入っているルビ関連のタグです。手作業でこれらを削除していくのは大変ですが、実はfindメソッドで取得した本文テキスト（main_textオブジェクト）はBeautiful Soup 4のTagオブジェクトとなっていて、このオブジェクトにはdecomposeメソッドが用意されています。このメソッドは特定のタグとその内容を削除するのに使えます。そこで、上で取り出したmain_textでルビ関連のタグ（の一部）を削除してみましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags_to_delete = main_text.find_all(['rp', 'rt'])\n",
    "for tag in tags_to_delete:\n",
    "    tag.decompose()\n",
    "print(main_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ここでは＜rp＞タグと＜rt＞タグの2つだけを削除の対象としています。それ以外はもちろん残ってしまうのですが、＜rb＞タグの内容は削除してしまっては困るもの（ルビを振る文字そのもの）ですから、これはそういうものだと思いましょう。＜ruby＞タグも同様で、これを削除してしまうとルビだけではなく、本文テキストの一部まで削除してしまいます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "タグもまだ残っています。これらはどうすればよいでしょう。Beautiful Soup 4のTagオブジェクトには「get_textメソッド」という便利なメソッドがあります。これは人が読めるようなテキストを抜き出すのに使えます（戻り値はBeautiful Soup 4のオブジェクトではなく、単なる文字列です）。実際に使ってみましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_text = main_text.get_text()\n",
    "print(main_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "タグは消えますが、ルビの情報である「（したい）」などがテキスト中に含まれていることに注目してください。これらは全角かっこ「（）」に囲まれたひらがなですから、正規表現を使って「main_text = re.sub('（[\\u3041-\\u309F]+）', '', main_text)」のようなことをすることで削除可能です。が、本文テキストとしてこのような文字の並びが登場する可能性はゼロとはいいきれません。そこで、＜rp＞タグと＜rt＞タグという情報を手がかりとして、削除してもよいものを前もって処理しておくことにしました。\n",
    "\n",
    "　ところで、先ほどの文章に「（見た目は）」とあるのに気が付いた方もいらっしゃるかもしれません。見た目とはどういうことでしょう。これはprint関数にmain_textを渡すのではなく、「main_text」とだけセルに入力して、Google Colab上で評価してみると分かります。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "「\\r」「\\n」「\\u3000」などの文字がmain_textオブジェクトに埋め込まれているのが分かります（最後の「\\u3000」は全角空白文字のコードポイント）。これは文字列のreplaceメソッドを使って削除してしまいましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_text = main_text.replace('\\r', '').replace('\\n', '').replace('\\u3000', '')\n",
    "main_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最後にエクスクラメーションマーク「！」と句点「。」の直後に改行を含めるようにします。これで1文ごとに改行されるようになります。といっても、そうしたいのではなく、最後にこれをsplitlinesメソッドで個々の文を要素とするリストを作成しておくためです"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "main_text = re.sub('([！。])', r'\\1\\n', main_text)  # 。と！で改行\n",
    "text_list = main_text.splitlines()\n",
    "print(text_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "関数にしておく"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from urllib import request\n",
    "import re\n",
    "\n",
    "def get_url_text(url=None):\n",
    "    if url == None:\n",
    "        return [\"\"]\n",
    "    response = request.urlopen(url)\n",
    "    soup = BeautifulSoup(response)\n",
    "    response.close()\n",
    "    main_text = soup.find('div', class_='main_text')\n",
    "    main_text = main_text.get_text()\n",
    "    main_text = main_text.replace('\\r', '').replace('\\n', '').replace('\\u3000', '')\n",
    "    main_text = re.sub('([！。])', r'\\1\\n', main_text)  # 。と！で改行\n",
    "    text_list = main_text.splitlines()\n",
    "    return text_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Wordcloudを用いた形態素解析"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WordCloudとは\n",
    "![](./utils/word_cloud.png)\n",
    "\n",
    "ニュースでこんなグラフをみたことはありませんか？\n",
    "\n",
    "WordCloudは、文章中で出現頻度が高い単語を複数選び出し、その頻度に応じた大きさで図示する手法。ウェブページやブログなどに頻出する単語を自動的に並べることなどを指す。文字の大きさだけでなく、色、字体、向きに変化をつけることで、文章の内容をひと目で印象づけることができる。(デジタル大辞泉の解説より)\n",
    "\n",
    "wordcloudで文字の出現頻度を可視化する方法を実装してみます。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "wordcloudはデフォルトでは日本語に対応していないので、適当な日本語フォントを読み込ませる必要があります。\n",
    "また、Google Colab には日本語の True Font が入っていないので、インストールします。このコマンドをコピペして実行します。\n",
    "!apt-get -y install fonts-ipafont-gothic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_list = get_url_text(url=\"https://www.aozora.gr.jp/cards/000148/files/773_14560.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from janome.tokenizer import Tokenizer\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Janome で単語単位に分割し、必要な単語だけ抜き出し、Word Cloud を実行します。\n",
    "単語単位に分割し、名詞（但し、非自立・代名詞・数を除く）のみを対象に抜き出し、words_wakati （単語が空白で区切られ改行コードはない状態）に保存します"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = Tokenizer()\n",
    "tokens = t.tokenize(\"\".join(text_list))\n",
    "word_list=[]\n",
    "for token in tokens:\n",
    "    word = token.surface\n",
    "    partOfSpeech = token.part_of_speech.split(',')[0]\n",
    "    partOfSpeech2 = token.part_of_speech.split(',')[1]\n",
    "     \n",
    "    word_list.append(word)\n",
    " \n",
    "words_wakati=\" \".join(word_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "コードをみて頂ければ分かると思います。参考に、# default 値を記載してあります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpath = './utils/GenShinGothic-Bold.ttf'  # 日本語フォント指\n",
    "wordcloud = WordCloud(\n",
    "    font_path=fpath,\n",
    "    width=900, height=600,   # default width=400, height=200\n",
    "    background_color=\"white\",   # default=”black”\n",
    "    max_words=500,   # default=200\n",
    "    min_font_size=4,   #default=4\n",
    "    collocations = False   #default = True\n",
    "    ).generate(words_wakati)\n",
    "plt.figure(figsize=(15,12))\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 表示させたくない単語の削除\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上記のワードクラウドを見ると、「する」「いる」のような、「そりゃ沢山使われるよね」という単語がデカデカと表示されています。\n",
    "そういった、「消したい単語」は、単語を連結する際に省いておく必要があります。\n",
    "\n",
    "具体的には、以下のように、単語が指定した語句一覧に含まれていたら文字列連結をやめる処理にすればOKです。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_list=[]\n",
    "for token in tokens:\n",
    "    word = token.surface\n",
    "    partOfSpeech = token.part_of_speech.split(',')[0]\n",
    "    partOfSpeech2 = token.part_of_speech.split(',')[1]\n",
    "    if word not in [\"それ\", \"ない\", \"する\", \"いる\"]:\n",
    "        word_list.append(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "また特定の品詞のみを使用することもできます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_list=[]\n",
    "for token in tokens:\n",
    "    word = token.surface\n",
    "    partOfSpeech = token.part_of_speech.split(',')[0]\n",
    "    partOfSpeech2 = token.part_of_speech.split(',')[1]\n",
    "    if partOfSpeech == \"名詞\":\n",
    "       if (partOfSpeech2 != \"非自立\") and (partOfSpeech2 != \"代名詞\") and (partOfSpeech2 != \"数\"):\n",
    "            word_list.append(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4 WordCloud作成+画像でマスク処理\n",
    "mask画像を使用すればニュースで見るようなWord Cloudを作成することも可能です。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n",
    "from wordcloud import ImageColorGenerator\n",
    "def get_wordcrowd_color_mask( text, img_color):\n",
    "    \n",
    "    wc = WordCloud(width=900,\n",
    "                   height=600,\n",
    "                   font_path=fpath,\n",
    "                   background_color=\"white\",\n",
    "                   mask=img_color,\n",
    "                   collocations=False, # 単語の重複しないように\n",
    "                  ).generate( text )\n",
    "\n",
    "    image_colors = ImageColorGenerator(img_color)\n",
    "\n",
    "    # show\n",
    "    plt.figure(figsize=(12,12))\n",
    "    plt.imshow(wc, # 元画像の色を使う\n",
    "               interpolation=\"bilinear\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "img_color = np.array(Image.open( './utils/mask1.png' ))\n",
    "get_wordcrowd_color_mask(words_wakati, img_color)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image, ImageOps\n",
    "# 画像を読み込んだ後、白黒反転した画像を作成\n",
    "img_color = Image.open('./utils/mask.jpg')\n",
    "im_invert = ImageOps.invert(img_color)\n",
    "im_invert = np.array(im_invert)\n",
    "get_wordcrowd_color_mask(words_wakati, im_invert)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 自然言語処理における前処理の種類\n",
    "以下スライド参照<br>\n",
    "![](./utils/自然言語処理/Slide35.jpg)\n",
    "![](./utils/自然言語処理/Slide36.jpg)\n",
    "![](./utils/自然言語処理/Slide37.jpg)\n",
    "![](./utils/自然言語処理/Slide38.jpg)\n",
    "![](./utils/自然言語処理/Slide39.jpg)\n",
    "![](./utils/自然言語処理/Slide40.jpg)\n",
    "![](./utils/自然言語処理/Slide45.jpg)\n",
    "![](./utils/自然言語処理/Slide46.jpg)\n",
    "![](./utils/自然言語処理/Slide47.jpg)\n",
    "![](./utils/自然言語処理/Slide48.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 単語をベクトル化とは？\n",
    "そもそもベクトルとは「大きさだけでなく、向きももった量」のことです。\n",
    "単語を「大きさだけでなく、向きももった量」にすると言う意味がよくわからないと思いますが、実際にやっていく実行していくなかで理解してもらいたいと思います。\n",
    "### 単語ベクトルとは\n",
    "単語ベクトルとは，単語を数百次元の実数値ベクトルとして表現したものであり，単語間の意味的な関係の強さをそれぞれの単語に対応するベクトルの成す角のコサインやユークリッド距離などを用いて定量化することを目的としています。<br>\n",
    "要はコンピュータで計算できないもの(単語)を計算できるようにすることを目的にしています。<br>\n",
    "また、単語ベクトルは単語の分散表現(Distributed Representation，Word Embedding）とも呼ばれ単語に留まらず文を表現させる研究も盛んに行われています。\n",
    "さまざまな単語ベクトル構成手法の内，昨今，注目されているのがニューラルネットワークに基づく学習手法です。<br>\n",
    "特に，Google の Mikolov らが提案した手法とそれを実装したツール word2vec2 は，大規模なコーパスで学習させることでベクトル空間内でking − man + woman を計算すると queen に近い値を出すといいたものです。\n",
    "今回はこれを皆さんに体験してもらおうと思います。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 単語の分散表現とは？\n",
    "単語の分散表現のメリットを理解するための比較対象として、単語の one-hot表現 についても説明します。\n",
    "#### one-hot表現\n",
    "文字や単語を、記号の世界から数値の世界に変換する手法として用いられるのが、「One-hotベクトル表現」です。One-hotベクトルとは、ベクトルのすべての要素のうちひとつだけが1であり、残りはすべて0であるベクトルを意味します。One-hotベクトル表現の言語処理への応用では、世の中に存在するすべての単語を、（0,0……,0,1,0,……0,0）のベクトルで表現します。これにより、単語とベクトルが一対一の関係になり、プログラムで処理できるようになります。このように、各概念を一対一対応で表現する手法は「局所表現」と呼ばれます。<br>\n",
    "しかしこの手法では、単語同士が関連していないため、同一単語であるかどうかの判定以外の処理を行うことは難しく、また世界に存在する数多くの言語のすべての単語を網羅するとなると、ベクトル数が膨大になり、計算時間が激増するという問題点がありました。<br>\n",
    "![](./utils/自然言語処理/Slide50.jpg)\n",
    "![](./utils/自然言語処理/Slide51.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 分散表現\n",
    "局所表現による言語処理の計算時間の膨大さという問題点を解決したのが、2000年に提案された、「分散表現」の言語処理への応用である単語分散表現の登場です。\n",
    "\n",
    "単語分散表現とは、「文字・単語をベクトル空間に埋め込み、その空間上のひとつの点として捉える」ことを指します。単語分散表現は、単語埋め込み（Word Embedding）とも呼ばれます。\n",
    "\n",
    "局所表現では、ある概念をほかの概念から完全に独立したものとして表現しています。一方、分散表現では、ある概念を表現する際に、ほかの概念との共通点や類似性と紐づけながら、ベクトル空間上に表現します。以下の例のベクトルの各要素は我々が理解できる食べ物に関連した概念ですが、ベクトルの各要素はアルゴリズムが勝手に作り出すものであり、人間に解釈できるものとは限りません。\n",
    "更に、それぞれの概念同士が関連し合っている故に、分散表現では異なる概念を表現するベクトル同士での計算が可能です。\n",
    "この分散表現は、人間が新しいことを記憶する際に、既に知っていることと関連させて記憶することや、あらゆる事象はさまざまな特徴で表現されうるといった、認知心理学や神経科学が人間の脳に関して持つ知見を応用し生まれたものです。\n",
    "\n",
    "![](./utils/自然言語処理/Slide52.jpg)\n",
    "![](./utils/自然言語処理/Slide53.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### なぜ自然言語処理にとって単語の分散表現は重要なのか？\n",
    "![](./utils/自然言語処理/Slide54.jpg)\n",
    "![](./utils/自然言語処理/Slide55.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 単語のベクトル化 word2vec\n",
    "![](./utils/自然言語処理/Slide56.jpg)\n",
    "![](./utils/自然言語処理/Slide57.jpg)\n",
    "![](./utils/自然言語処理/Slide58.jpg)\n",
    "![](./utils/自然言語処理/Slide59.jpg)\n",
    "![](./utils/自然言語処理/Slide60.jpg)\n",
    "![](./utils/自然言語処理/Slide61.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-hot表現を作ってみる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import janome.tokenizer #形態素解析器(日本語を単語に分割するライブラリ)\n",
    "\n",
    "# Documents\n",
    "d_01 = \"私は今朝おにぎりを食べました。\"\n",
    "d_02 = \"今日の朝はあめでした。犬の散歩には行きませんでした。\"\n",
    "d_03 = \"論文を読むのは楽しい。\"\n",
    "d_04 = \"あめは美味しい。\"\n",
    "\n",
    "# 分かち書き(Tokenを見出し語に戻す)\n",
    "tokenizer = janome.tokenizer.Tokenizer()\n",
    "print([token.base_form for token in tokenizer.tokenize(d_01)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vocabularyを作る\n",
    "tokens_01 = [token.base_form for token in tokenizer.tokenize(d_01)]\n",
    "tokens_02 = [token.base_form for token in tokenizer.tokenize(d_02)]\n",
    "tokens_03 = [token.base_form for token in tokenizer.tokenize(d_03)]\n",
    "tokens_04 = [token.base_form for token in tokenizer.tokenize(d_04)]\n",
    "vocabulary = list(set(tokens_01+tokens_02+tokens_03+tokens_04))\n",
    "print(vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 単語IDを表示\n",
    "for i in range(len(vocabulary)):\n",
    "    print(\"token ID : {}, token : {}\".format(i,vocabulary[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot化\n",
    "import sklearn.preprocessing\n",
    "vocabulary_onehot = sklearn.preprocessing.label_binarize(vocabulary,classes=vocabulary)\n",
    "\n",
    "for token, onehotvec in zip(vocabulary,vocabulary_onehot):\n",
    "    print(\"one-hot vector : {}, token : {}\".format(onehotvec,token))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 任意のOne-hot表現を取り出す\n",
    "token_index = vocabulary.index(\"私\")\n",
    "print(\"「私」のOne-hot表現は {}\".format(vocabulary_onehot[token_index]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "今回は分割と見出し語化(「食べ」→「食べる」等)のみ行いました。<br>\n",
    "実際には以下の様な処理が入ることが有ります。\n",
    "- クリーニング(htmlタグ除去等)\n",
    "- 正規化(半角全角、大文字小文字、表記ゆれの統一等)\n",
    "- ストップワード除去(頻出で意味のない単語。the等)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One-hot表現の問題点① : 次元が大きすぎる\n",
    "\n",
    "- 上記の例ではCorpusが小さかったので、Vocabularyの大きさも20程度でした。\n",
    "- 実際の日本語wikipediaコーパスは150万語程度です。\n",
    "- ある単語が専有するメモリは莫大なものになります。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "# 150万次元のOne-hotベクトルを作ってみて、メモリを確認する\n",
    "memory_check_list = [0] * 1500000\n",
    "memory_check_list[0] = 1\n",
    "memory = sys.getsizeof(memory_check_list)\n",
    "memory = memory / (1000*1000)\n",
    "print(\"ある単語が専有するメモリ : {:0.1f}MB\".format(memory))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One-hot表現の問題点② : 意味をエンコードできない\n",
    "\n",
    "- ベクトルは内積やコサイン類似度といった手法で「近さ」を定量化出来ます。\n",
    "- しかしOne-hot表現では、異なる単語ベクトルの内積・コサイン類似度は0になってしまいます。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inu_vec = vocabulary_onehot[vocabulary.index(\"犬\")]\n",
    "kesa_vec = vocabulary_onehot[vocabulary.index(\"今朝\")]\n",
    "asa_vec = vocabulary_onehot[vocabulary.index(\"朝\")]\n",
    "\n",
    "# 「犬」と「朝」の内積\n",
    "print(\"「犬」と「朝」の距離 : {}\".format(np.dot(inu_vec,asa_vec)))\n",
    "\n",
    "# 「今朝」と「朝」の内積\n",
    "print(\"「今朝」と「朝」の距離 : {}\".format(np.dot(kesa_vec,asa_vec)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "もし意味を表現できるようになれば、以下の事ができます。\n",
    "\n",
    "- 意味の近い単語ほど、ベクトルは近くなる(cos類似度が1に近くなる)。\n",
    "- 単語の足し引きで他の単語を表現できる。\n",
    "    -「パリ」ー「フランス」＋「ドイツ」≒「ベルリン」 (首都の意味をエンコード)\n",
    "    - 「おじさん」ー「男」＋「女」≒「おばさん」 (性別の意味をエンコード)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### カウントベースと推論ベース\n",
    "単語の分散表現を獲得する手法としては、大きく分けてカウントベースの手法と推論ベースの手法の二つがあります。カウントベースの手法は周囲の単語の頻度によって単語を表現する方法で、コーパス全体の統計データから単語の分散表現を獲得します。一方で、推論ベースの手法はニューラルネットワークを用いて少量の学習サンプルをみながら重みを繰り返し更新する手法です。Word2Vecは後者に該当します。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### word2vec\n",
    "\n",
    "-「次元が大きすぎる」「意味をエンコードできない」というOne-hot表現の弱点を克服する手法がword2vecです。\n",
    "- word2vecはCBOWとSkip-gramという2種類に分類出来ます。\n",
    "\n",
    "|分類 |\t意味|\n",
    "|----|----|\n",
    "|CBOW |\t周辺の単語から、ある単語を予測する|\n",
    "|Skip-gram| \tある単語から、周辺の単語を予測する|\n",
    "\n",
    "\n",
    "「周辺」のサイズはウィンドウサイズと呼び、ハイパーパラメータです。<br>\n",
    "「ある場所に入る単語の確率分布は、その周辺の単語によって決定される」という分布仮説という考え方があります。<br>\n",
    "だから「周辺の単語」から「ある単語」を予測する事ができます。(CBOW)<br>\n",
    "逆も然りです。(Skip-gram)<br>\n",
    "\n",
    "-   「ある単語」のベクトル表現を、150万次元から300次元に削減しています。\n",
    "-   この「300次元のベクトル」はスパース(One-hotの様に、多くが0の意味)ではなく、密なベクトルです。\n",
    "-    このやり方で得られたベクトルは、意味をエンコードできている事が報告されています。<br>\n",
    "それでは実行してみましょう。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "分かち書きには`janome`を使用します。\n",
    "#### Analyzerモジュール\n",
    "Analyzerの主な前処理と後処理は以下の通りです。\n",
    "\n",
    "前処理\n",
    "文章の抽出、不要な文字列の削除(HTMLタグなど)、文字種の統一(英字は全て英小文字にするなど)、スペルミス・変換ミスなどによる表記ゆらぎの補正などをします。\n",
    "\n",
    "後処理\n",
    "分かち書き後の字句(トークン)を対象としており、数字の置換(数字の名詞は全て0に置き換えるなど)、特定の品詞のみの抽出をします。\n",
    "\n",
    "Analyzerは、以下の3つを組み合わせて使用します。\n",
    "\n",
    "- 文字の正規化などの前処理を行うCharFilter\n",
    "- 形態素解析後の後処理を行うTokenFilter\n",
    "- 分かち書きされたトークン単位で処理するTokenFilter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CharFilter\n",
    "| UnicodeNormalizeCharFilter | Unicodeを正規化することで表記ゆれを吸収する<br> - 引数に’NFC’, ‘NFKC’, ‘NFD’, ‘NFKD’を指定可能<br> - デフォルトは’NFKC’で、全角→半角などの変換が行われる|\n",
    "| -----|------|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### テキストから単語のリストを作成する方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from janome.analyzer import Analyzer\n",
    "from janome.tokenfilter import POSKeepFilter\n",
    "\n",
    "main_text = \"\"\"\n",
    "Analyzerの主な前処理と後処理は以下の通りです。\n",
    "前処理\n",
    "文章の抽出、不要な文字列の削除(HTMLタグなど)、文字種の統一(英字は全て英小文字にするなど)、スペルミス・変換ミスなどによる表記ゆらぎの補正などをします。\n",
    "後処理\n",
    "分かち書き後の字句(トークン)を対象としており、数字の置換(数字の名詞は全て0に置き換えるなど)、特定の品詞のみの抽出をします。\n",
    "Analyzerは、以下の3つを組み合わせて使用します。\n",
    "\"\"\" \n",
    "\n",
    "token_filters = [POSKeepFilter(['名詞', '代名詞', \"動詞\"])]\n",
    "a = Analyzer(token_filters=token_filters)\n",
    "# 単語のリストを作成する。\n",
    "sentences = [tok.surface for tok in a.analyze(main_text)]\n",
    "sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### モデルの学習\n",
    "gensimを用いてWord2vecのモデルの作成を行います。下記がモデルを作成するに当たっての主要なパラメータになります。\n",
    "| パラメータ | 説明 |\n",
    "|----------|-------|\n",
    "|sg\t| 1ならskip-gramで0ならCBOWで学習する|\n",
    "|size\t| 何次元の分散表現を獲得するかを指定 |\n",
    "|window\t| コンテクストとして認識する前後の単語数を指定|\n",
    "|min_count\t| 指定の数以下の出現回数の単語は無視する|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## データ準備"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 小説を5つ用意\n",
    "text_list1 = get_url_text(\"https://www.aozora.gr.jp/cards/000035/files/1567_14913.html\")\n",
    "text_list2 = get_url_text(\"https://www.aozora.gr.jp/cards/000879/files/43016_16836.html\")\n",
    "text_list3 = get_url_text(\"https://www.aozora.gr.jp/cards/000888/files/33205_26197.html\")\n",
    "text_list4 = get_url_text(\"https://www.aozora.gr.jp/cards/000888/files/51164_66323.html\")\n",
    "text_list5 = get_url_text(\"https://www.aozora.gr.jp/cards/001123/files/42931_35771.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# janomeを使ってテキストのリストから単語のりすとを生成\n",
    "# Janomeのロード\n",
    "from janome.tokenizer import Tokenizer\n",
    "# Tokenizerインスタンスの生成 \n",
    "t = Tokenizer()\n",
    "# テキストを引数として、形態素解析の結果、名詞・動詞・形容詞(原形)のみを配列で抽出する関数を定義 \n",
    "def extract_words(sum_list):\n",
    "    sentences = []\n",
    "    for text_list in sum_list:\n",
    "        token_list = []\n",
    "        for text_line in text_list:\n",
    "            tokens = t.tokenize(text_line)\n",
    "            for token in tokens:\n",
    "                if token.part_of_speech.split(',')[0] in['名詞']:\n",
    "                    token_list.append(token.base_form)\n",
    "        sentences.append(token_list)\n",
    "    return sentences\n",
    "word_list = extract_words([text_list1, text_list2, text_list3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.word2vec import Word2Vec\n",
    "\n",
    "#モデルを作る\n",
    "model = Word2Vec(word_list, min_count=1, size=30)\n",
    "#モデルを保存する\n",
    "model.save('model/kokoro.model')\n",
    "print('saved at \"kokoro.model\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Word2Vec.load(\"model/kokoro.model\") # gensim形式のモデルをロードします"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ボキャブラリを確認"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv.vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 単語のベクトルを見る"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 0.00666874, -0.02655591, -0.06156563, -0.06872889, -0.11217778,\n",
       "        -0.17082544,  0.04338928,  0.06854654, -0.05634882,  0.05920975,\n",
       "        -0.06325503, -0.06752525,  0.18078369, -0.07915374,  0.07642227,\n",
       "         0.11696416, -0.03348758, -0.13711472,  0.15528458, -0.10116944,\n",
       "        -0.0241161 , -0.02158193,  0.03992549, -0.05645756,  0.08966302,\n",
       "         0.05592261, -0.12762943, -0.06977747,  0.2168577 ,  0.06273849],\n",
       "       dtype=float32),\n",
       " (30,))"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 単語のベクトルを見る\n",
    "model.wv[\"王\"],model.wv[\"王\"].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ある単語と類似している単語を見る\n",
    "similar_wordsにはリストになっていて、その要素はtupleとなっている。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "今 1.00\n",
      "ところ 1.00\n",
      "人間 1.00\n",
      "よう 1.00\n",
      "人 1.00\n",
      "僕 1.00\n",
      "日 1.00\n",
      "それ 0.99\n",
      "もの 0.99\n"
     ]
    }
   ],
   "source": [
    "# 猫と類似している単語を見る\n",
    "similar_words = model.wv.most_similar(positive=[\"王\"], topn=9)\n",
    "print(*[\" \".join([v, str(\"{:.2f}\".format(s))]) for v, s in similar_words], sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('区切', 0.2922549247741699)\n",
      "('異端', 0.23880639672279358)\n",
      "('本人', 0.21319395303726196)\n",
      "('行い', 0.19466367363929749)\n",
      "('借り物', 0.187569260597229)\n",
      "('好意', 0.1537606120109558)\n",
      "('従弟', 0.1470605731010437)\n",
      "('定', 0.14625807106494904)\n",
      "('成長', 0.14529377222061157)\n",
      "('警告', 0.13281014561653137)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "results = model.wv.most_similar(negative=['王', 'メロス'])\n",
    "for result in results:\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 分散表現・単語のリストを取得\n",
    "word_vectors = model.wv.vectors\n",
    "index2word = model.wv.index2word\n",
    "# indexを取得\n",
    "nouns_id = [i for i, n in enumerate(index2word)]\n",
    "\n",
    "# 単語を抽出\n",
    "word_vectors = word_vectors[nouns_id]\n",
    "index2word = [index2word[i] for i in nouns_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorboardX import SummaryWriter\n",
    "import torch\n",
    "writer = SummaryWriter('./runs')\n",
    "writer.add_embedding(torch.FloatTensor(word_vectors), metadata=index2word)\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorboardによる可視化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    }
   ],
   "source": [
    "!tensorboard --logdir=./runs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 単語埋め込みのみを使うシンプルな文章埋め込み\n",
    "\n",
    "![](utils\\swem_01.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "文章に対する固定次元の分散表現を得る手法としては、doc2vecやSkip-thoughts、テキスト間の含意関係を学習することで分散表現を得るinfersent、最近では強力な言語モデルとなったBERTといった方法があります。これらの手法は、単語ベクトルに加えて文章ベクトルを得るためのニューラルネットワーク自体を、大規模コーパスから学習させる必要があります。\n",
    "\n",
    "より単純ながらも後続タスクへの精度がでる文章埋め込みの計算方法として、追加学習やパラメータチューニングを必要とせず単語埋め込みだけを利用するSWEM (Simple Word-Embedding-based Methods) という手法があります。\n",
    "\n",
    "方法\n",
    "SWEMでは以下の4つの方法が提案されています。\n",
    "- SWEM-aver：単語の分散表現に対してaverage poolingする\n",
    "- SWEM-max：単語の分散表現に対してmax poolingする\n",
    "- SWEM-concat：SWEM-averとSWEM-maxの結果を結合する\n",
    "- SWEM-hier：n-gramのように固定長のウィンドウでaverage-poolingした結果に対してmax poolingする\n",
    "\n",
    "これらは基本的に、文章に含まれる単語の分散表現全体に対して、どういう操作で固定時点のベクトルに集約するかといった操作の違いでしかありません。それぞれのaverage poolingやmax poolingは、element-wiseにaverageやmaxを取ります。Out-of-Vocabulary (OOV) な単語に対しては、[-0.01, 0.01]の範囲の一様乱数を用いて初期化します。なお、aver, max, concatに関してはパラメータはありませんが、SWEM-hierはn-gramのウィンドウの幅nを決める必要があります。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ちなみに、結局のところどれが一番いいのかという話ですが、論文中の評価ではタスク/データ依存という結果になっており、一概にどれが良いかは断定できないようです。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import MeCab\n",
    "class MeCabTokenizer():\n",
    "    def __init__(self, mecab_args=\"\"):\n",
    "        self.tagger = MeCab.Tagger(mecab_args)\n",
    "        self.tagger.parse(\"\")\n",
    "\n",
    "    def tokenize(self, text):\n",
    "        return self.tagger.parse(text).strip().split(\" \")\n",
    "tokenizer = MeCabTokenizer(\"-O wakati\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.word2vec import Word2Vec\n",
    "model = Word2Vec.load(\"model/kokoro.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class SWEM():\n",
    "    \"\"\"\n",
    "    Simple Word-Embeddingbased Models (SWEM)\n",
    "    https://arxiv.org/abs/1805.09843v1\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, w2v, tokenizer, oov_initialize_range=(-0.01, 0.01)):\n",
    "        self.w2v = w2v\n",
    "        self.tokenizer = tokenizer\n",
    "        self.vocab = set(self.w2v.wv.vocab.keys())\n",
    "        self.embedding_dim = self.w2v.wv.vector_size\n",
    "        self.oov_initialize_range = oov_initialize_range\n",
    "\n",
    "        if self.oov_initialize_range[0] > self.oov_initialize_range[1]:\n",
    "            raise ValueError(\"Specify valid initialize range: \"\n",
    "                             f\"[{self.oov_initialize_range[0]}, {self.oov_initialize_range[1]}]\")\n",
    "\n",
    "    def get_word_embeddings(self, text):\n",
    "        np.random.seed(abs(hash(text)) % (10 ** 8))\n",
    "\n",
    "        vectors = []\n",
    "        for word in self.tokenizer.tokenize(text):\n",
    "            if word in self.vocab:\n",
    "                vectors.append(self.w2v.wv[word])\n",
    "            else:\n",
    "                vectors.append(np.random.uniform(self.oov_initialize_range[0],\n",
    "                                                 self.oov_initialize_range[1],\n",
    "                                                 self.embedding_dim))\n",
    "        return np.array(vectors)\n",
    "\n",
    "    def average_pooling(self, text):\n",
    "        word_embeddings = self.get_word_embeddings(text)\n",
    "        return np.mean(word_embeddings, axis=0)\n",
    "\n",
    "    def max_pooling(self, text):\n",
    "        word_embeddings = self.get_word_embeddings(text)\n",
    "        return np.max(word_embeddings, axis=0)\n",
    "\n",
    "    def concat_average_max_pooling(self, text):\n",
    "        word_embeddings = self.get_word_embeddings(text)\n",
    "        return np.r_[np.mean(word_embeddings, axis=0), np.max(word_embeddings, axis=0)]\n",
    "\n",
    "    def hierarchical_pooling(self, text, n):\n",
    "        word_embeddings = self.get_word_embeddings(text)\n",
    "\n",
    "        text_len = word_embeddings.shape[0]\n",
    "        if n > text_len:\n",
    "            raise ValueError(f\"window size must be less than text length / window_size:{n} text_length:{text_len}\")\n",
    "        window_average_pooling_vec = [np.mean(word_embeddings[i:i + n], axis=0) for i in range(text_len - n + 1)]\n",
    "\n",
    "        return np.max(window_average_pooling_vec, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "swem = SWEM(model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.01740239,  0.00467173,  0.00468411,  0.00182449, -0.00559801,\n",
       "        0.00601738,  0.03942473,  0.07938815,  0.001332  ,  0.06327808,\n",
       "       -0.0055727 ,  0.004028  ,  0.19817368,  0.00045112,  0.10696049,\n",
       "        0.14082498,  0.00745657,  0.00902755,  0.18223785,  0.00955089,\n",
       "        0.00883863,  0.00757619,  0.03650183,  0.00609387,  0.11258099,\n",
       "        0.04559575, -0.00173469,  0.01143377,  0.23378519,  0.05813621])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"メロスは激怒した\"\n",
    "# SWEM-aver\n",
    "swem.average_pooling(text)\n",
    "# SWEM-max\n",
    "swem.max_pooling(text)\n",
    "# SWEM-concat\n",
    "swem.concat_average_max_pooling(text)\n",
    "# SWEM-hier\n",
    "swem.hierarchical_pooling(text, n=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spacy+GiNZAを使ったの日本語処理\n",
    "spaCyは高度な自然言語処理を行うためのライブラリです。\n",
    "自然言語処理では対象とする言語（日本語や英語）によって必要な処理や複雑度が変わるのですが、spaCyは多言語対応を意識して設計・開発されており、そのアーキテクチャから学べることも多く非常に良くできたライブラリです。\n",
    "\n",
    "GiNZAは日本語の自然言語処理を行うためのライブラリでリクルートと国語研が共同で開発したライブラリです。\n",
    "GiNZAはspaCyのAPIを使用して学習されており、spaCyからモデルをロードして使用することができます。\n",
    "\n",
    "Google ColaboratoryにはspaCyがデフォルトでインストールされています。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n",
      "Collecting ja-ginza\n",
      "  Downloading ja_ginza-5.1.0-py3-none-any.whl (59.1 MB)\n",
      "Collecting spacy<3.3.0,>=3.2.3\n",
      "  Downloading spacy-3.2.3-cp37-cp37m-win_amd64.whl (11.5 MB)\n",
      "Collecting sudachidict-core>=20210802\n",
      "  Downloading SudachiDict-core-20211220.tar.gz (9.1 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting sudachipy<0.7.0,>=0.6.2\n",
      "  Downloading SudachiPy-0.6.3-cp37-cp37m-win_amd64.whl (1.0 MB)\n",
      "Collecting ginza<5.2.0,>=5.1.0\n",
      "  Downloading ginza-5.1.1-py3-none-any.whl (20 kB)\n",
      "Collecting plac>=1.3.3\n",
      "  Downloading plac-1.3.4-py2.py3-none-any.whl (22 kB)\n",
      "Requirement already satisfied: typing-extensions<4.0.0.0,>=3.7.4 in c:\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.3->ja-ginza) (3.7.4.3)\n",
      "Collecting thinc<8.1.0,>=8.0.12\n",
      "  Downloading thinc-8.0.15-cp37-cp37m-win_amd64.whl (1.0 MB)\n",
      "Collecting typer<0.5.0,>=0.3.0\n",
      "  Downloading typer-0.4.0-py3-none-any.whl (27 kB)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.3->ja-ginza) (2.25.1)\n",
      "Requirement already satisfied: setuptools in c:\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.3->ja-ginza) (52.0.0.post20210125)\n",
      "Collecting langcodes<4.0.0,>=3.2.0\n",
      "  Downloading langcodes-3.3.0-py3-none-any.whl (181 kB)\n",
      "Collecting catalogue<2.1.0,>=2.0.6\n",
      "  Downloading catalogue-2.0.7-py3-none-any.whl (17 kB)\n",
      "Requirement already satisfied: jinja2 in c:\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.3->ja-ginza) (2.11.3)\n",
      "Collecting cymem<2.1.0,>=2.0.2\n",
      "  Downloading cymem-2.0.6-cp37-cp37m-win_amd64.whl (35 kB)\n",
      "Collecting murmurhash<1.1.0,>=0.28.0\n",
      "  Downloading murmurhash-1.0.6-cp37-cp37m-win_amd64.whl (20 kB)\n",
      "Requirement already satisfied: numpy>=1.15.0 in c:\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.3->ja-ginza) (1.18.1)\n",
      "Collecting spacy-loggers<2.0.0,>=1.0.0\n",
      "  Downloading spacy_loggers-1.0.1-py3-none-any.whl (7.0 kB)\n",
      "Collecting wasabi<1.1.0,>=0.8.1\n",
      "  Downloading wasabi-0.9.0-py3-none-any.whl (25 kB)\n",
      "Collecting pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4\n",
      "  Downloading pydantic-1.8.2-cp37-cp37m-win_amd64.whl (1.9 MB)\n",
      "Collecting srsly<3.0.0,>=2.4.1\n",
      "  Downloading srsly-2.4.2-cp37-cp37m-win_amd64.whl (450 kB)\n",
      "Collecting blis<0.8.0,>=0.4.0\n",
      "  Downloading blis-0.7.7-cp37-cp37m-win_amd64.whl (6.6 MB)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.3->ja-ginza) (4.56.0)\n",
      "Collecting preshed<3.1.0,>=3.0.2\n",
      "  Downloading preshed-3.0.6-cp37-cp37m-win_amd64.whl (108 kB)\n",
      "Collecting pathy>=0.3.5\n",
      "  Downloading pathy-0.6.1-py3-none-any.whl (42 kB)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.3->ja-ginza) (20.9)\n",
      "Collecting spacy-legacy<3.1.0,>=3.0.8\n",
      "  Downloading spacy_legacy-3.0.9-py2.py3-none-any.whl (20 kB)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\anaconda3\\lib\\site-packages (from catalogue<2.1.0,>=2.0.6->spacy<3.3.0,>=3.2.3->ja-ginza) (3.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in c:\\anaconda3\\lib\\site-packages (from packaging>=20.0->spacy<3.3.0,>=3.2.3->ja-ginza) (2.4.7)\n",
      "Collecting smart-open<6.0.0,>=5.0.0\n",
      "  Downloading smart_open-5.2.1-py3-none-any.whl (58 kB)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.3->ja-ginza) (2021.5.30)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in c:\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.3->ja-ginza) (4.0.0)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.3->ja-ginza) (2.10)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.3->ja-ginza) (1.25.11)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\anaconda3\\lib\\site-packages (from typer<0.5.0,>=0.3.0->spacy<3.3.0,>=3.2.3->ja-ginza) (7.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\anaconda3\\lib\\site-packages (from jinja2->spacy<3.3.0,>=3.2.3->ja-ginza) (1.1.1)\n",
      "Building wheels for collected packages: sudachidict-core\n",
      "  Building wheel for sudachidict-core (setup.py): started\n",
      "  Building wheel for sudachidict-core (setup.py): finished with status 'done'\n",
      "  Created wheel for sudachidict-core: filename=SudachiDict_core-20211220-py3-none-any.whl size=71565332 sha256=c3c1d39d8424e73176f418a79511e37af422feed781ff20e1bf50c22443a319e\n",
      "  Stored in directory: c:\\users\\hideaki\\appdata\\local\\pip\\cache\\wheels\\17\\6d\\f6\\f9451cbfc76ffdf985af12239191ca20721f1da5aba5005eb0\n",
      "Successfully built sudachidict-core\n",
      "Installing collected packages: murmurhash, cymem, catalogue, wasabi, typer, srsly, smart-open, pydantic, preshed, blis, thinc, sudachipy, spacy-loggers, spacy-legacy, pathy, langcodes, sudachidict-core, spacy, plac, ginza, ja-ginza\n",
      "  Attempting uninstall: smart-open\n",
      "    Found existing installation: smart-open 4.1.2\n",
      "    Uninstalling smart-open-4.1.2:\n",
      "      Successfully uninstalled smart-open-4.1.2\n",
      "  Attempting uninstall: pydantic\n",
      "    Found existing installation: pydantic 1.7.3\n",
      "    Uninstalling pydantic-1.7.3:\n",
      "      Successfully uninstalled pydantic-1.7.3\n",
      "  Attempting uninstall: sudachipy\n",
      "    Found existing installation: SudachiPy 0.4.9\n",
      "    Uninstalling SudachiPy-0.4.9:\n",
      "      Successfully uninstalled SudachiPy-0.4.9\n",
      "  Attempting uninstall: sudachidict-core\n",
      "    Found existing installation: SudachiDict-core 20200722\n",
      "    Uninstalling SudachiDict-core-20200722:\n",
      "      Successfully uninstalled SudachiDict-core-20200722\n",
      "Successfully installed blis-0.7.7 catalogue-2.0.7 cymem-2.0.6 ginza-5.1.1 ja-ginza-5.1.0 langcodes-3.3.0 murmurhash-1.0.6 pathy-0.6.1 plac-1.3.4 preshed-3.0.6 pydantic-1.8.2 smart-open-5.2.1 spacy-3.2.3 spacy-legacy-3.0.9 spacy-loggers-1.0.1 srsly-2.4.2 sudachidict-core-20211220 sudachipy-0.6.3 thinc-8.0.15 typer-0.4.0 wasabi-0.9.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.3.1; however, version 22.0.4 is available.\n",
      "You should consider upgrading via the 'c:\\anaconda3\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "!pip install ja-ginza"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ja_ginzaは国語研のデータセットを畳み込みニューラルネットワーク（CNN）で依存関係ラベリングや単語依存構造解析などのタスクを学習させたモデルになります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "# Languageクラス 変数名をnlpで宣言するのが一般的（spaCy推奨）\n",
    "nlp = spacy.load('ja_ginza')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 単語依存構造の可視化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "spaCyではdisplacyモジュールが用意されており、Docクラスの解析結果を可視化（HTML/SVG出力）することができます。<br>\n",
    "手始めに解析したDocクラスから単語依存構造の可視化してみましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"ja\" id=\"4d61d5e2b7a845e584d4507c353f04c1-0\" class=\"displacy\" width=\"1250\" height=\"362.0\" direction=\"ltr\" style=\"max-width: none; height: 362.0px; color: #000000; background: #ffffff; font-family: Arial; direction: ltr\">\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"272.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">錦織</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">PROPN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"272.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"200\">圭</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"200\">PROPN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"272.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"350\">選手</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"350\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"272.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"500\">は</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"500\">ADP</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"272.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"650\">テニス</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"650\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"272.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"800\">が</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"800\">ADP</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"272.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"950\">大好き</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"950\">ADJ</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"272.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1100\">です。</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1100\">AUX</tspan>\n",
       "</text>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-4d61d5e2b7a845e584d4507c353f04c1-0-0\" stroke-width=\"2px\" d=\"M62,227.0 62,177.0 347.0,177.0 347.0,227.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-4d61d5e2b7a845e584d4507c353f04c1-0-0\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">compound</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M62,229.0 L58,221.0 66,221.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-4d61d5e2b7a845e584d4507c353f04c1-0-1\" stroke-width=\"2px\" d=\"M212,227.0 212,202.0 344.0,202.0 344.0,227.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-4d61d5e2b7a845e584d4507c353f04c1-0-1\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">compound</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M212,229.0 L208,221.0 216,221.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-4d61d5e2b7a845e584d4507c353f04c1-0-2\" stroke-width=\"2px\" d=\"M362,227.0 362,152.0 950.0,152.0 950.0,227.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-4d61d5e2b7a845e584d4507c353f04c1-0-2\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">dislocated</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M362,229.0 L358,221.0 366,221.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-4d61d5e2b7a845e584d4507c353f04c1-0-3\" stroke-width=\"2px\" d=\"M362,227.0 362,202.0 494.0,202.0 494.0,227.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-4d61d5e2b7a845e584d4507c353f04c1-0-3\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">case</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M494.0,229.0 L498.0,221.0 490.0,221.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-4d61d5e2b7a845e584d4507c353f04c1-0-4\" stroke-width=\"2px\" d=\"M662,227.0 662,177.0 947.0,177.0 947.0,227.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-4d61d5e2b7a845e584d4507c353f04c1-0-4\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M662,229.0 L658,221.0 666,221.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-4d61d5e2b7a845e584d4507c353f04c1-0-5\" stroke-width=\"2px\" d=\"M662,227.0 662,202.0 794.0,202.0 794.0,227.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-4d61d5e2b7a845e584d4507c353f04c1-0-5\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">case</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M794.0,229.0 L798.0,221.0 790.0,221.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-4d61d5e2b7a845e584d4507c353f04c1-0-6\" stroke-width=\"2px\" d=\"M962,227.0 962,202.0 1094.0,202.0 1094.0,227.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-4d61d5e2b7a845e584d4507c353f04c1-0-6\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">aux</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1094.0,229.0 L1098.0,221.0 1090.0,221.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "</svg></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from spacy import displacy\n",
    "text = '錦織圭選手はテニスが大好きです。'\n",
    "doc = nlp(text)\n",
    "# 依存関係の可視化（jupyter=TrueとすることでNotebook上で表示できる）\n",
    "displacy.render(doc, style=\"dep\", options={\"compact\":True},  jupyter=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### POS(Part-of-speech)タグとは\n",
    "動詞や形容詞といった品詞のタグのこと\n",
    "### Universal POS tags\n",
    "|POS|\t意味\t|単語例|\n",
    "|------|---------|--------|\n",
    "|ADJ|\t形容詞\t|big, old, green, incomprehensible, first, second, third|\n",
    "|ADP|\t設置詞\t|in, to, during|\n",
    "|ADV|\t副詞|\tvery, well, exactly|\n",
    "|AUX| 助動詞|\thas(done), is(doing), will(do), was(done), got(done), should(do), must(do)|\n",
    "|CCONJ|\t接続詞|\tand, or, but|\n",
    "|DET|\t限定詞|\tthe, a, an, this, that, my, your, a few, a little, one, ten, all, both, another, such, what|\n",
    "|INTJ|\t間投詞|\tpsst, ouch, bravo, hello, well, you know, execuse me|\n",
    "|NOUN|\t名詞|\tgirl, cat, tree, air, beauty|\n",
    "|NUM|\t数詞|\t0, 1000, 3.14, one, two, seventy-seven, I, II, III, IV, V, MMXIV|\n",
    "|PART|\t助詞|\t's, not|\n",
    "|PRON|\t代名詞|\tI, you, he, it, they, myself, yourself, who, what, somebody, anything, everybody, nothing|\n",
    "|PROPN|\t固有名詞|\tMary, John, London, NATO, HBO|\n",
    "|PUNCT|\t句読点|\t.(ピリオド), ,(カンマ), ()(括弧)|\n",
    "|SCONJ|\t連結詞|\tthat, if, while|\n",
    "|SYM|\tシンボル|\t$, %, §, ©, +, −, ×, ÷, =, <, >, :-)(顔文字), :joy: (絵文字), kei.0324@example.com, http://example.com/|\n",
    "|VERB|\t動詞|\trun. eat, runs. ate, runnning, eating|\n",
    "|X|\tその他|\t上記品詞に当てはまらない単語|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 係り受けタグとは\n",
    "係り受け解析時に使用される、単語や句、節などの関係性を表すタグのことです。\n",
    "### Dependency Relations\n",
    "\n",
    "|係り受けタグ|\t意味\t|例文|\n",
    "|----------|----------|----------|\n",
    "|acl\t|名詞の節修飾語|\tI have a <b>parakeet</b> named cookie .|\n",
    "|advcl\t|副詞節修飾語\t|He <b>talked</b> to him in order to secure the account .|\n",
    "|advmod\t|副詞修飾語\t|Where do you want to <b>go</b> later ?|\n",
    "|amod\t|形容詞修飾語\t|Sam eats red <b>meat</b> .|\n",
    "|appos\t|同格\t|<b>Sam</b> , my brother , arrived .|\n",
    "|aux\t|助動詞\t|He should <b>leave</b> .|\n",
    "|case\t|格表示\t|The office of the <b>Chair</b> .|\n",
    "|cc\t|等位接続詞\t|Bill is big and <b>honest</b> .|\n",
    "|ccomp\t|補文\t|He <b>says</b> you like to swim .|\n",
    "|clf\t|類別詞\t|Take <b>this</b>> CLF bus .|\n",
    "|compound\t|複合名詞\t|A phone <b>book</b> .|\n",
    "|conj\t|結合詞\t|Bill is <b>big</b> and honest .|\n",
    "|cop\t|連結詞\t|Ivan is the best <b>dancer</b> .|\n",
    "|csubj\t|主部\t|What she said is <b>interesting</b> .|\n",
    "|dep\t|不明な依存関係\t|正確に依存関係を表せない場合に使用する|\n",
    "|det\t|限定詞\t|The <b>man</b> is here .|\n",
    "|discourse\t|談話要素\t|Iguazu <b>is</b> in Argentina :)|\n",
    "|dislocated\t|転置\t|象 は 鼻 が <b>長い</b> 。|\n",
    "|expl\t|嘘辞\t|It is <b>clear</b> that we should decline .|\n",
    "|fixed\t|固定複数単語表現\t|He cried <b>because</b> of you .|\n",
    "|flat\t|同格複数単語表現\t|<b>Mr.</b> Smith|\n",
    "|goeswith\t|1単語分割表現\t|They come here <b>with</b> - out legal permission .|\n",
    "|iobj\t|関節目的語\t|She <b>gave</b> me a raise .|\n",
    "|list\t|リスト表現\t|Long <b>Lines</b> , Silly Rules , Rude Staff , Ok Food .|\n",
    "|mark\t|接続詞\t|He says that you <b>like</b> to swim|\n",
    "|nmod\t|名詞修飾語\t|The office of the <b>Chair</b> .|\n",
    "|nsubj\t|主語名詞\t|There <b>is</b> a ghost in the room .|\n",
    "|nummod\t|数詞修飾語\t|Sam spent forty <b>dollars</b> .|\n",
    "|obj\t|目的語\t|She <b>gave</b> me a raise .|\n",
    "|obl\t|斜格名詞\t|<b>Give</b> the toys to the children .|\n",
    "|orphan\t|独立関係\t|Marie won gold and <b>Peter</b> bronze|\n",
    "|parataxis\t|並列\t|The guy , John said , <b>left</b> early in the morning .|\n",
    "|punct\t|句読点\t|We have apples , <b>pears</b> , <b>oranges</b> , and <b>bananas</b> .|\n",
    "|reparandum\t|単語として認識されない単語表現\t|Go to the righ- to the <b>left</b> .|\n",
    "|root\t|文の根(ROOTは人為的に付加される)\t|<b>ROOT</b> I love French fries .|\n",
    "|vocative\t|発声関係\t|Guys , <b>take</b> it easy!|\n",
    "|xcomp\t|補体\t|Sue <b>asked</b> George to respond to her offer .|\n",
    "\n",
    "※ 例文では係り受け元を 太字 、係り受け先を 斜字 にしています。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### エンティティ抽出\n",
    "エンティティとは実世界のオブジェクトを指す単語のことです。\n",
    "spaCyでは先ほどの依存構造の可視化と同様にdisplacyモジュールを使って、テキスト中に含まれるエンティティをハイライトして表示することができます。\n",
    "（styleの引数をentにするだけです。）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
       "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    錦織圭\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">Person</span>\n",
       "</mark>\n",
       "\n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    選手\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">Position_Vocation</span>\n",
       "</mark>\n",
       "は\n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    テニス\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">Sport</span>\n",
       "</mark>\n",
       "が大好きです。</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# エンティティの可視化（jupyter=TrueとすることでNotebook上で表示できる）\n",
    "displacy.render(doc, style=\"ent\", options={\"compact\":True},  jupyter=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "錦織圭 <class 'spacy.tokens.span.Span'>\n",
      "選手 <class 'spacy.tokens.span.Span'>\n",
      "テニス <class 'spacy.tokens.span.Span'>\n"
     ]
    }
   ],
   "source": [
    "for ent in doc.ents:\n",
    "  print(ent.text, type(ent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "錦織 <class 'spacy.tokens.token.Token'>\n",
      "圭 <class 'spacy.tokens.token.Token'>\n",
      "選手 <class 'spacy.tokens.token.Token'>\n",
      "テニス <class 'spacy.tokens.token.Token'>\n"
     ]
    }
   ],
   "source": [
    "for ent in doc.ents:\n",
    "  for token in ent:\n",
    "    print(token.text, type(token))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 名詞句の抽出\n",
    "文に含まれる名詞を把握するだけで、なんとなくでも内容を把握することができるので、テキスト文の内容把握したいようなユースケースでは、自然言語処理によってテキスト文から名詞だけを抽出するという操作がよく行われます。\n",
    "\n",
    "spaCyではこのようなユースケースに応えるように名詞句を簡単に抽出できます。\n",
    "\"句\"は\"単語\"よりも上位の概念です。\n",
    "文の中で品詞の役割を果たす単位が\"句\"になります。\n",
    "\n",
    "先ほどの文章とは別のテキスト文「錦織圭選手は偉大なテニス選手です。」で考えます。\n",
    "\n",
    "spaCyではDocクラスを生成した段階で名詞句を抽出しnoun_chunksプロパティに保持しています。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "錦織圭選手 <class 'spacy.tokens.span.Span'>\n",
      "偉大なテニス選手 <class 'spacy.tokens.span.Span'>\n"
     ]
    }
   ],
   "source": [
    "doc2 = nlp('錦織圭選手は偉大なテニス選手です。')\n",
    "\n",
    "# noun_chunksでテキスト文に含まれる名詞句を取り出す\n",
    "for chunk in doc2.noun_chunks:\n",
    "  print(chunk.text, type(chunk))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "錦織 名詞-固有名詞-人名-姓 <class 'spacy.tokens.token.Token'>\n",
      "圭 名詞-固有名詞-人名-名 <class 'spacy.tokens.token.Token'>\n",
      "選手 名詞-普通名詞-一般 <class 'spacy.tokens.token.Token'>\n",
      "テニス 名詞-普通名詞-一般 <class 'spacy.tokens.token.Token'>\n",
      "選手 名詞-普通名詞-一般 <class 'spacy.tokens.token.Token'>\n"
     ]
    }
   ],
   "source": [
    "# 品詞タグから名詞の単語を抽出する\n",
    "for token in doc2:\n",
    "  if token.pos_ in ['NOUN', 'PROPN']: # NOUNが名詞、PROPNが固有名詞\n",
    "    print(token.text, token.tag_, type(token))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b10430241691b249f3f5968ff1d5f373841e9f0f1c6e818b42478bb30bce80ad"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
