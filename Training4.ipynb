{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 自然言語処理 100本ノック"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 自然言語とは\n",
    "英語や日本語の様に人間が自然発生的に使用してきた言語の事を自然言語処理といいます。<br>\n",
    "これに対し、プログラミング言語のような人工言語は形式言語と呼んで区別されます。<br>\n",
    "自然言語 <-> 形式言語"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 自然言語処理とは"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "自然言語を計算機(コンピューター)で処理する事を指します。\n",
    "計算機で処理を行う場合、言葉を計算機が計算しやすい様に、数値に変換する必要があります。\n",
    "画像や音声の場合は、ピクセル数や音の高さ等で表現(数値化)することができます。\n",
    "それに対して、自然言語の場合は、目的によって最適な表現方法(数値化)が異なる為、数値化が難しいとされています。\n",
    "簡単な方法としては、単語をあるなし(0, 1)で数値化する方法があります。\n",
    "これでは、単語ごとの意味が加味されていませんので、現在は、単語や文章の特徴を数値化して計算機で計算を行うことが主流となっています。\n",
    "具体的な特徴として、文章に出現する単語の頻出頻度や近年は単語同士の関連性が使用されており、また、特徴量の算出方法(アルゴリズム)も複数存在し、目的によって最適な方法が複数取られています。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 自然言語処理の活用"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "自然言語処理と機械学習を組み合わせることで、一例として以下のような事(用語としてはタスクといいます)がこなせます。\n",
    "\n",
    "- 文章分類\n",
    "- 自動要約\n",
    "- 機械翻訳\n",
    "- 文章生成\n",
    "- 著者推定\n",
    "- 自動ダグ生成"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 自然言語処理の基本的な流れ\n",
    "自然言語を計算機で処理する際の基本的な流れは以下のようなものです。\n",
    "ここでは、日本語に対しての処理の流れを説明します。\n",
    "\n",
    "形態素解析\n",
    "形態素解析器を使用して文を最小単位に分割する。また、各単語を品詞区分に紐付ける。\n",
    "今日はいい天気 -> \"今日\", \"は\", \"いい\", \"天気\"\n",
    "\"今日\": 名詞, \"は\": 助詞, \"いい\": 形容詞, \"天気\":名詞\n",
    "構文解析\n",
    "形態素解析を行った単語同士の関係性を解析する\n",
    "詳細(自然言語（日本語）処理)\n",
    "意味解析\n",
    "構文解析をした結果、複数の候補がある場合、意味が伝わる構文を選択する\n",
    "詳細(自然言語（日本語）処理)\n",
    "文脈解析\n",
    "複数の文章にまたがる構文解析+意味解析を行う\n",
    "近年は、(特に英語ですが)形態素解析を行い、その単語をベクトル化(数値化)するだけで様々なタスクでよい精度が出ています。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 以下スライドによる解説\n",
    "\n",
    "![](utils/自然言語処理/Slide1.jpg)\n",
    "![](utils/自然言語処理/Slide4.jpg)\n",
    "![](utils/自然言語処理/Slide5.jpg)\n",
    "![](utils/自然言語処理/Slide6.jpg)\n",
    "![](utils/自然言語処理/Slide7.jpg)\n",
    "![](utils/自然言語処理/Slide8.jpg)\n",
    "![](utils/自然言語処理/Slide9.jpg)\n",
    "![](utils/自然言語処理/Slide10.jpg)\n",
    "![](utils/自然言語処理/Slide11.jpg)\n",
    "![](utils/自然言語処理/Slide12.jpg)\n",
    "![](utils/自然言語処理/Slide13.jpg)\n",
    "![](utils/自然言語処理/Slide14.jpg)\n",
    "![](utils/自然言語処理/Slide15.jpg)\n",
    "![](utils/自然言語処理/Slide16.jpg)\n",
    "![](utils/自然言語処理/Slide17.jpg)\n",
    "![](utils/自然言語処理/Slide18.jpg)\n",
    "![](utils/自然言語処理/Slide28.jpg)\n",
    "![](utils/自然言語処理/Slide29.jpg)\n",
    "![](utils/自然言語処理/Slide30.jpg)\n",
    "![](utils/自然言語処理/Slide31.jpg)\n",
    "![](utils/自然言語処理/Slide32.jpg)\n",
    "![](utils/自然言語処理/Slide33.jpg)\n",
    "![](utils/自然言語処理/Slide34.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "本項では自然言語処理の基礎知識と応用について体験してもらいます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Python正規表現の基本"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pythonではパッケージreを使って正規表現の実装します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 関数を直接使用"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "re.matchやre.subなどの関数を使います。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 第1引数が正規表現パターン(検索語句)、第2引数が検索対象\n",
    "result = re.match('Hel', 'Hellow python')\n",
    "\n",
    "print(result)\n",
    "# <_sre.SRE_Match object; span=(0, 3), match='Hel'>\n",
    "\n",
    "print(result.group())\n",
    "# Hel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 コンパイルして使用"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "正規表現パターンをコンパイルした後にmatchやsubなどの関数を使用します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# あらかじめ正規表現パターンをコンパイル\n",
    "regex = re.compile('Hel')\n",
    "\n",
    "result = regex.match('Hellow python')\n",
    "\n",
    "print(result)\n",
    "# <_sre.SRE_Match object; span=(0, 3), match='Hel'>\n",
    "\n",
    "print(result.group())\n",
    "# Hel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2種類の使い分け\n",
    "複数の正規表現パターンを何度も使う場合はコンパイル方式を使います。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">re.compile() を使い、結果の正規表現オブジェクトを保存して再利用するほうが、一つのプログラムでその表現を何回も使うときに効率的です。\n",
    "re.compile() やモジュールレベルのマッチング関数に渡された最新のパターンはコンパイル済みのものがキャッシュされるので、一度に正規表現を少ししか使わないプログラムでは正規表現をコンパイルする必要はありません。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 正規表現パターン(検索語句)の定義"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 raw文字列でエスケープシーケンス無効"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "raw文字列(raw string)は正規表現固有のトピックではありませんが、使うことでエスケープシーケンスを無効にできます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('a\\tb\\nA\\tB')\n",
    "print(r'a\\tb\\nA\\tB')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "正規表現パターン内でバックスラッシュなどに対してエスケープシーケンスを書きたくないのでraw文字列を使います"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = re.match(r'\\d', '329')\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 トリプルクォートとre.VERBOSEで改行・コメント・空白無視"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'''トリプルクォート(\"\"\"でも可能)で囲むことにより、正規表現パターン中に改行を使うことができます(改行がなくても問題なし)。\n",
    "re.VERBOSEを渡すことで、空白やコメントを正規表現パターンから除外できます。\n",
    "リプルクォートとre.VERBOSEで非常に読みやすくなります\n",
    "以下のような正規表現パターンを記述すると見やすいです。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = re.compile(r'''\\d +  # the integral part\n",
    "                   \\.    # the decimal point\n",
    "                   \\d *  # some fractional digits''', re.VERBOSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "トリプルクォートに関しては、記事「Pythonで文字列生成（引用符、strコンストラクタ）」に詳しくかかれています。\n",
    "\n",
    "ちなみにcompileのパラメータflagsで複数のコンパイルフラグを使いたい場合は、単純に+(加算)してやればOKです。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = re.compile(r'''\\d''', re.VERBOSE+re.MULTILINE)\n",
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 OR条件（文字単体）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "文字列の一部で、いずれかにヒットするみたいなOR条件的なパターンを使うには[]の括弧を利用します（文字の集合といった具合に呼ばれます。英語だとcharacter classなど）。\n",
    "\n",
    "例えば、「特定の文字部分が猫もしくは犬のどちらかにヒットする」といった条件のパターンは以下のように設定できます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "re.search(\n",
    "    pattern=r'吾輩は[猫犬]である。',\n",
    "    string='吾輩は猫である。名前はまだ無い。')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "re.search(\n",
    "    pattern=r'吾輩は[猫犬]である。',\n",
    "    string='吾輩は犬である。名前はまだ無い。')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "re.search(\n",
    "    pattern=r'吾輩は[猫犬]である。',\n",
    "    string='吾輩は兎である。名前はまだ無い。')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "re.search(\n",
    "    pattern=r'吾輩は[猫犬兎]である。',\n",
    "    string='吾輩は兎である。名前はまだ無い。')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 OR条件（文字列）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "先ほどは[]の括弧を使って文字単体のOR条件を試しました。文字単体ではなく文字列のOR条件を設定したい場合は条件の範囲を()の括弧で囲み、文字列間に|を挟みます（英語だとAlternation）。\n",
    "\n",
    "例えば、猫であると犬だよという文字列でOR条件を設定したい場合には以下のような書き方になります。猫犬両方の文字列でヒットします。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "re.search(\n",
    "    pattern=r'吾輩は(猫である|犬だよ)。',\n",
    "    string='吾輩は猫である。名前はまだ無い。')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "re.search(\n",
    "    pattern=r'吾輩は(猫である|犬だよ)。',\n",
    "    string='吾輩は犬だよ。名前はまだ無い。')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "re.search(\n",
    "    pattern=r'吾輩は(猫である|犬だよ|兎です)。',\n",
    "    string='吾輩は兎です。名前はまだ無い。')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6 特殊文字\n",
    "| 文字 | 説明　| 備考 | 例 | マッチする| マッチしない | \n",
    "|------|-------|------|-----|---------|----------|\n",
    "|\\d\t|数字\t|[0-9]と同じ| | | \n",
    "| \\D\t|数字以外\t|[^0-9]と同じ|||\n",
    "|\\s |\t空白文字\t|[\\t\\n\\r\\f\\v]と同じ|||\t\t\t\n",
    "|\\S\t|空白文字以外\t|[^\\t\\n\\r\\f\\v]と同じ|||\t\t\t\n",
    "|\\w\t|英数文字と下線\t|[a-zA-Z0-9_]と同じ\t\t\t\n",
    "|\\W\t|英数文字以外\t|[\\a-zA-Z0-9_]と同じ\t\t\t\n",
    "|\\A\t|文字列の先頭\t|^と類似\t\t\t\n",
    "|\\Z\t|文字列の末尾\t|$と類似\t\t\t\n",
    "|\\b\t|単語の境界(スペース)\t\t\t\t\n",
    "|.\t|任意の一文字\t|-|\t1.3|\t123, 133|\t1223|\n",
    "|^\t|文字列の先頭\t|-|\t^123|\t1234|\t0123|\n",
    "|$\t|文字列の末尾\t|-|\t123$|\t0123|\t1234|\n",
    "|*\t|０回以上の繰り返し\t|-|\t12*|\t1, 12, 122\t|11, 22\n",
    "|+\t|１回以上の繰り返し\t|-|\t12+\t|12, 122\t|1, 11, 22\n",
    "|?\t|０回または１回\t|-|\t12? |1, 12|\t122|\n",
    "|{m}\t|m回の繰り返し|\t-|\t1{3}|\t111\t|11, 1111|\n",
    "|{m,n}\t|m〜n回の繰り返し\t|-\t|1{2, 3}\t|11, 111\t|1, 1111|\n",
    "|[]\t|集合\t|[^5]とすると5以外\t|[1-3]\t|1, 2, 3\t|4, 5|\n",
    "| &#124; |\t和集合(or)\t|-\t|1&#124;2\t|1, 2\t|3|\n",
    "|()\t|グループ化\t|-|\t(12)+|\t|12, 1212\t|1, 123|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. マッチ関数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以下の関数をよく使います。\n",
    "\n",
    "|関数\t|目的|\n",
    "|----|-----|\n",
    "|match\t|文字列の先頭で正規表現とマッチするか判定|\n",
    "|search\t|正規表現がどこにマッチするか検索|\n",
    "|findall|マッチする部分文字列を全て探しリストとして返します|\n",
    "|sub\t|文字列置換|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 matchとsearch\n",
    "文字列の先頭でのみのマッチするのがre.matchで文字列中の位置にかかわらずマッチするのがre.search。詳しくは公式の「search() vs. match()」を参照。両者とも最初のパターンのみを返します(2回目以降にマッチしたものを返さない)。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "re.match(\"c\", \"abcdef\")    # 先頭が\"c\"でないのでマッチしない"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "re.search(\"c\", \"abcdef\")   # マッチする"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "結果はgroupに入っています。group(0)に結果がすべて入っていて、グループ化した検索結果は連番で1から入っています。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " m = re.match(r\"(\\w+) (\\w+)\", \"Isaac Newton, physicist\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " m.group(0)       # The entire match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.group(1)       # The first parenthesized subgroup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.group(2)       # The second parenthesized subgroup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " m.group(1, 2)    # Multiple arguments give us a tuple."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 findall \n",
    "パターンにマッチした全ての文字列をリスト形式で返すのがfindall。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"He was carefully disguised but captured quickly by police.\"\n",
    "re.findall(r\"\\w+ly\", text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "()を使ってキャプチャ対象を指定できますが、複数指定した場合は以下のようになります。グループごとにタプルで返ってきます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(re.findall(r'''(1st)(2nd)''', '1st2nd1st2nd'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 sub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "文字置換をします。引数の順に1. 正規表現パターン、2. 置換後の文字列、3. 置換対象文字列です。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " re.sub(r'置換前', '置換後', '置換前 対象外 置換前')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.　形態素解析"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 形態素解析とは "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "形態素解析とは、コンピュータを利用して機械的な方法で文を形態素（言葉で意味を持つ最小の単位）に区切る技術のことを言います。\n",
    "\n",
    "我々の身近な言語としては、日本語、英語があります。\n",
    "\n",
    "この２つの言語は、当然ですが、使用する言葉と文法が異なります。\n",
    "\n",
    "例えば、日本語で「これはペンです。」という文は、英語だと「This is a pen.」です。\n",
    "\n",
    "英語は、”This”, “is”, “a”, “pen”という風に、単語と単語の間に空白が存在しますが、日本語は単語と単語の間に空白が存在しません。\n",
    "\n",
    "このように、単語と単語の間に空白を入れる書き方を分かち書きといいます。\n",
    "\n",
    "そして、分かち書きされていない日本語の文章を機械に理解させるためには、単語分割を行うとともに品詞を明確にする必要があるのです。\n",
    "\n",
    "なぜなら文章は、決められた文法法則に従い単語に付与されている品詞を並べて構成されているからです。\n",
    "\n",
    "機械に対して、単語の意味、品詞、文法を理解させなければ意味を解釈させることもできなければ、文を作成させることもできません。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 形態素解析ツール"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "日本語の主な形態素解析ツールには以下のようなものがあります。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "・MeCab\n",
    "・ChaSen\n",
    "・JUMAN\n",
    "・Janome"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 形態素解析の実装 | MeCab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install mecab-python3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以下のような出力が得られたら成功です。\n",
    "\n",
    "\n",
    "Collecting mecab-python3\n",
    "  Downloading https://files.pythonhosted.org/packages/c1/72/20f8f60b858556fdff6c0376b480c230e594621fff8be780603ac9c47f6a/mecab_python3-1.0.3-cp37-cp37m-manylinux1_x86_64.whl (487kB)\n",
    "     |████████████████████████████████| 491kB 8.6MB/s \n",
    "Installing collected packages: mecab-python3\n",
    "Successfully installed mecab-python3-1.0.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "次に、辞書データとしてUniDicをインストールします。\n",
    "UniDicは国立国語研究所が開発している辞書です。\n",
    "以下を入力し実行してください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install unidic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以下のような出力が得られたら成功です。\n",
    "Collecting unidic\n",
    "  Downloading https://files.pythonhosted.org/packages/86/04/c18832fd9959a78fc60eeaa9e7fb37ef31a250e8645cc2897eb1f07939ee/unidic-1.0.3.tar.gz\n",
    "Requirement already satisfied: requests<3.0.0,>=2.22.0 in /usr/local/lib/python3.7/dist-packages (from unidic) (2.23.0)\n",
    "Requirement already satisfied: tqdm<5.0.0,>=4.41.1 in /usr/local/lib/python3.7/dist-packages (from unidic) (4.41.1)\n",
    "Requirement already satisfied: wasabi<1.0.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from unidic) (0.8.2)\n",
    "Requirement already satisfied: plac<2.0.0,>=1.1.3 in /usr/local/lib/python3.7/dist-packages (from unidic) (1.1.3)\n",
    "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.22.0->unidic) (1.24.3)\n",
    "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.22.0->unidic) (2020.12.5)\n",
    "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.22.0->unidic) (2.10)\n",
    "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.22.0->unidic) (3.0.4)\n",
    "Building wheels for collected packages: unidic\n",
    "  Building wheel for unidic (setup.py) ... done\n",
    "  Created wheel for unidic: filename=unidic-1.0.3-cp37-none-any.whl size=5497 sha256=78ab4afc1982544644d7782dac00a36b55b71aceeb11cb7d0d692f01bd995f10\n",
    "  Stored in directory: /root/.cache/pip/wheels/d3/26/e2/fb76c79fd14391eb994eab021c9129c24814125298e1e5b96a\n",
    "Successfully built unidic\n",
    "Installing collected packages: unidic\n",
    "Successfully installed unidic-1.0.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "更に、辞書データもダウンロードします。\n",
    "\n",
    "以下のコードを実行しましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m unidic download"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以下のような出力が得られたら成功です。\n",
    "download url: https://cotonoha-dic.s3-ap-northeast-1.amazonaws.com/unidic.zip\n",
    "Dictionary version: 2.3.0+2020-10-08\n",
    "Downloading UniDic v2.3.0+2020-10-08...\n",
    "unidic.zip: 100% 608M/608M [00:22<00:00, 27.3MB/s]\n",
    "Finished download.\n",
    "Downloaded UniDic v2.3.0+2020-10-08 to /usr/local/lib/python3.7/dist-packag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 「私は形態素解析を学んでいます」という文章を解析します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import MeCab\n",
    "import unidic\n",
    "mecab = MeCab.Tagger()\n",
    "print(mecab.parse(\"私は形態素解析を学んでいます。\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "出力された内容を確認しましょう。出力される形態素解析の結果は、左から順に以下の通りとなります。\n",
    "\n",
    "表層形（surface）（文章中で使用されている単語）\n",
    "品詞（part_of_speech）\n",
    "品詞細分類1〜3（part_of_speech）\n",
    "活用型（infl_type）\n",
    "活用形（infl_form）\n",
    "原形（base_form）（文章中で使用されている単語の原形）\n",
    "読み（reading）\n",
    "発音（phonetic）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5 MeCabを用いて分かち書きをしたい場合"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "分かち書きをしたい場合は、Tagger()オブジェクトの出力モードに('-Owakati')を指定すればOKです。\n",
    "\n",
    "('-Owakati')を指定することで、品詞などを付与せず、形態素ごとに区切りの空白を入れることができます。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import MeCab\n",
    "import unidic\n",
    "mecab = MeCab.Tagger('-Owakati')\n",
    "print(mecab.parse(\"私は形態素解析を学んでいます。\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tagger()オブジェクトの出力モードには、以下も指定できます。\n",
    "\n",
    "- Oyomi: 読みのみを出力\n",
    "- Ochasen: ChaSen互換形式\n",
    "- Odump: 全ての情報を出力\n",
    "\n",
    "実行したい処理に応じて使い分けましょう。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.6 形態素解析の実装 | Janome"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "まずは、janomeをインストールしましょう。Google Colaboratoryのセルに以下を入力し実行すればOKです。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install janome"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以下のような出力が得られれば成功です。\n",
    "Collecting janome\n",
    "  Downloading https://files.pythonhosted.org/packages/a8/63/98858cbead27df7536c7e300c169da0999e9704d02220dc6700b804eeff0/Janome-0.4.1-py2.py3-none-any.whl (19.7MB)\n",
    "     |████████████████████████████████| 19.7MB 50.0MB/s \n",
    "Installing collected packages: janome\n",
    "Successfully installed janome-0.4.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.7 janomeを使った形態素解析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from janome.tokenizer import Tokenizer\n",
    "\n",
    "t = Tokenizer()\n",
    "tokens = t.tokenize(\"私は形態素解析を学んでいます。\")\n",
    "for token in tokens:\n",
    "  print(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "出力された内容は、MeCabと同様に、左から順に以下の通りとなります。\n",
    "\n",
    "表層形（surface）（文章中で使用されている単語）\n",
    "品詞（part_of_speech）\n",
    "品詞細分類1〜3（part_of_speech）\n",
    "活用型（infl_type）\n",
    "活用形（infl_form）\n",
    "原形（base_form）（文章中で使用されている単語の原形）\n",
    "読み（reading）\n",
    "発音（phonetic）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.8 Janomeを用いて分かち書きをしたい場合"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Janomeで分かち書きをしたい場合は、tokenize()メソッドの引数にwakati=Trueを指定すると、分かち書きのみを行います。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from janome.tokenizer import Tokenizer\n",
    "\n",
    "t = Tokenizer()\n",
    "tokens = t.tokenize(\"私は形態素解析を学んでいます。\", wakati=True)\n",
    "for token in tokens:\n",
    "  print(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.9　テキストファイルを読み込んで形態素解析を行う"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'data/wagahaiwa_nekodearu.txt'\n",
    "sentences = []\n",
    "\n",
    "tagger = MeCab.Tagger(\"-Owakati\")\n",
    "morphs = []\n",
    "with open(filename, mode='r', encoding=\"ShiftJIS\") as f:\n",
    "  for line in f:  # 1行ずつ読込\n",
    "    if line != 'EOS\\n':  # 文末以外：形態素解析情報を辞書型に格納して形態素リストに追加\n",
    "      fields = line.split('\\t')\n",
    "      sentences.append(tagger.parse(fields[0]))\n",
    "sentences[:4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 スクレイピングで青空文庫からデータを取得してみる\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "今回はスクレイピングで青空文庫からデータを取得をしてみます。\n",
    "### Webスクレイピングとは\n",
    "Webスクレイピングとは、Webページから情報を取得することを指します。Pythonを用いることで、取得だけでなく、取得した情報をExcelやGoogleスプレッドシートなどに整理し、利用できるようにすることも可能です。\n",
    "WebスクレイピングをするにはPythonとWebの基礎知識が必要になりますが、決して難しいものではありません。原理を理解することで、自分で使いやすいシステムを構築することができます。\n",
    "\n",
    "- Pythonで使えるWebスクレイピングのライブラリ<br>\n",
    "    PythonにはWebスクレイピングに使えるライブラリが用意されています。今回は代表的な3つのライブラリについて紹介していきます。\n",
    "- BeautifulSoup<br>\n",
    "    HTMLやXMLからデータを引き出せるライブラリです。Pythonでクローラーを作成する際によく使用されるライブラリですが、BeautifulSoup単体ではスクレイピングはできないため、HTTP通信ができるモジュールやCSVにエクスポートする他のライブラリと組み合わせて使用します。\n",
    "- Scrapy<br>\n",
    "    クローラーを実装・運用するために必要となる機能を持つ、アプリケーション全体を実装するためのフレームワークです。Webスクレイピング用に設計されましたが、APIを使用したデータ抽出や汎用クローラーとして使用することも可能です。\n",
    "- Selenium<br>\n",
    "    Webブラウザの操作を自動化するフレームワークです。本来はWebアプリケーションのUIテストを自動化するために開発されましたが、ブラウザの操作をコードで記述して自動化できる利便性の高さからタスクやWebサイトのクローリングなどに転用されています。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Webスクレイピングの注意点\n",
    "\n",
    "Webスクレイピングは、情報を収集するためにWebサイトに頻繁にアクセスします。アクセスする頻度によっては、Webサイトが設置されているサーバに大きな負荷をかけ、他のユーザーがアクセスしにくくなったり、サーバがダウンしてしまう、いわゆるDOS攻撃（Denial-of-service attack）になってしまうケースもあります。\n",
    "\n",
    "for分やwhile文で情報を取得することは禁止します。\n",
    "\n",
    "Webスクレイピングをおこなう際には、DOS攻撃にならないよう注意し、アクセスする間隔や頻度を調整するように気を付けましょう。悪意の有無に関わらず、DOS攻撃はサーバ負荷が大きく、アクセス先のWebサイトが設置されているサーバが共有の場合、最悪のケースでは他のユーザーを保護するためサイトがサーバから削除されることもあります。\n",
    "\n",
    "\n",
    "あくまで収集する情報を提供してもらっているという意識を忘れず、相手に迷惑をかけないプログラミングを心がけましょう。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 PythonでWebスクレイピング\n",
    "#### urllib.requestを利用したWebスクレイピング\n",
    "PythonにはURLを扱うためのモジュールとして、いくつかのモジュールをまとめたurllibモジュールパッケージが標準で付属しています。今回はこの標準モジュールのうち、urllib.reguestモジュールを利用してWebスクレイピングをおこないます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib import request\n",
    "response = request.urlopen('https://www.aozora.gr.jp/index.html')\n",
    "content = response.read()\n",
    "response.close()\n",
    "html = content.decode()\n",
    "\n",
    "title = html.split('<title>')[1].split('</title')[0]\n",
    "print(title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "まずurllib.request.urlopen関数でURLをオープンします。この関数でURLをオープンすると、サーバからはhttp.clientモジュールで定義されているHTTPResponseクラスのオブジェクトが返送されます。\n",
    "続いてreadメソッドを使用してWebページの内容（ソースコード）を取得し、URLをクローズします。\n",
    "ここまでの操作で取得したページの内容はbytesオブジェクト（バイト列）になっているため、decodeメソッドで文字列（str）にデコードします。\n",
    "最後に、文字列として取得できたデータから、今回はタイトルタグを取得するため、文字列操作でタイトルタグを検索して取得し、出力します。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BeautifulSoupを使用したWebスクレイピング\n",
    "ここからは、BeautifulSoupを使用したWebスクレイピングのサンプルコードを見ていきましょう。BeautifulSoupを使用すると、標準のurllibモジュールを使用するよりも簡潔なコードでWebスクレイピングを構築できます。\n",
    "\n",
    "前項で解説したとおり、BeautifulSoupは単体ではHTTPへの通信機能を持たないため、別のライブラリやパッケージと組み合わせて使用します。今回は「requests」というリクエスト用のパッケージを読み込み、URLを渡すことでWebページを読み込みます。\n",
    "\n",
    "まずrequestsとBeautifulSoupのライブラリをインポートします。続いて今回取得したいWebサイトのURLをrequestsのgetメソッドで展開してコンテンツを取得します。\n",
    "\n",
    "取得したコンテンツをresponseに格納してBeautifulSoupに渡し、responseの内容を解析します。最後に解析した内容をfindメソッドで検索して、get_textでテキストを取得し、出力します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "response = requests.get('https://www.aozora.gr.jp/index.html')\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "title = soup.find('title').get_text()\n",
    "print(title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 小説のデータを抽出してみる"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "データの取得にはBeautiful Soup 4を使います。まずは夏目漱石の小説を幾つか取得するコードを完成させていきましょう。\n",
    "[リンク](https://www.aozora.gr.jp/index_pages/person148.html)\n",
    "\n",
    "![](./utils/natume_souseki.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "今回は「こころ」を取得してみようと思います。国語の授業で読んだことがある人もいると思います。<br>\n",
    "リンクをクリックすると、次のようなページが表示されます。<br>\n",
    "![](./utils/kokoro.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "取得可能なファイルの種類には3つありますが、ここでは［ファイル種別］が［XHTMLファイル］となっているファイルを取得することにします。<br>\n",
    "このリンク（773_14560.html）をクリックすると以下のように作品が表示されます。<br>\n",
    "![](./utils/こころ.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "urllib.requestモジュールが提供するurlopen関数を使って、このページの内容を取得して、これをBeautiful Soup 4に入力すれば、さまざまな操作が可能なオブジェクトが手に入ります。\n",
    "ここまでの処理を実際に行うのが、以下のコードです。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from urllib import request\n",
    "\n",
    "url = 'https://www.aozora.gr.jp/cards/000074/files/427_19793.html'\n",
    "response = request.urlopen(url)\n",
    "soup = BeautifulSoup(response)\n",
    "response.close()\n",
    "\n",
    "print(soup)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "文テキストだけではなく、さまざまなタグが含まれています。そこで、まずは<div>タグ（class属性が\"main_text\"）となっている部分だけを取り出しましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_text = soup.find('div', class_='main_text')\n",
    "print(main_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "これで小説の本文テキストだけが得られましたが、気になるのは山ほど入っているルビ関連のタグです。手作業でこれらを削除していくのは大変ですが、実はfindメソッドで取得した本文テキスト（main_textオブジェクト）はBeautiful Soup 4のTagオブジェクトとなっていて、このオブジェクトにはdecomposeメソッドが用意されています。このメソッドは特定のタグとその内容を削除するのに使えます。そこで、上で取り出したmain_textでルビ関連のタグ（の一部）を削除してみましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags_to_delete = main_text.find_all(['rp', 'rt'])\n",
    "for tag in tags_to_delete:\n",
    "    tag.decompose()\n",
    "print(main_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ここでは＜rp＞タグと＜rt＞タグの2つだけを削除の対象としています。それ以外はもちろん残ってしまうのですが、＜rb＞タグの内容は削除してしまっては困るもの（ルビを振る文字そのもの）ですから、これはそういうものだと思いましょう。＜ruby＞タグも同様で、これを削除してしまうとルビだけではなく、本文テキストの一部まで削除してしまいます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "タグもまだ残っています。これらはどうすればよいでしょう。Beautiful Soup 4のTagオブジェクトには「get_textメソッド」という便利なメソッドがあります。これは人が読めるようなテキストを抜き出すのに使えます（戻り値はBeautiful Soup 4のオブジェクトではなく、単なる文字列です）。実際に使ってみましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_text = main_text.get_text()\n",
    "print(main_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "タグは消えますが、ルビの情報である「（したい）」などがテキスト中に含まれていることに注目してください。これらは全角かっこ「（）」に囲まれたひらがなですから、正規表現を使って「main_text = re.sub('（[\\u3041-\\u309F]+）', '', main_text)」のようなことをすることで削除可能です。が、本文テキストとしてこのような文字の並びが登場する可能性はゼロとはいいきれません。そこで、<rp>タグと<rt>タグという情報を手がかりとして、削除してもよいものを前もって処理しておくことにしました。\n",
    "\n",
    "　ところで、先ほどの文章に「（見た目は）」とあるのに気が付いた方もいらっしゃるかもしれません。見た目とはどういうことでしょう。これはprint関数にmain_textを渡すのではなく、「main_text」とだけセルに入力して、Google Colab上で評価してみると分かります。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "「\\r」「\\n」「\\u3000」などの文字がmain_textオブジェクトに埋め込まれているのが分かります（最後の「\\u3000」は全角空白文字のコードポイント）。これは文字列のreplaceメソッドを使って削除してしまいましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_text = main_text.replace('\\r', '').replace('\\n', '').replace('\\u3000', '')\n",
    "main_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最後にエクスクラメーションマーク「！」と句点「。」の直後に改行を含めるようにします。これで1文ごとに改行されるようになります。といっても、そうしたいのではなく、最後にこれをsplitlinesメソッドで個々の文を要素とするリストを作成しておくためです"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "main_text = re.sub('([！。])', r'\\1\\n', main_text)  # 。と！で改行\n",
    "text_list = main_text.splitlines()\n",
    "print(text_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "関数にしておく"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from urllib import request\n",
    "import re\n",
    "\n",
    "def get_url_text(url=None):\n",
    "    if url == None:\n",
    "        return [\"\"]\n",
    "    response = request.urlopen(url)\n",
    "    soup = BeautifulSoup(response)\n",
    "    response.close()\n",
    "    main_text = soup.find('div', class_='main_text')\n",
    "    main_text = main_text.get_text()\n",
    "    main_text = main_text.replace('\\r', '').replace('\\n', '').replace('\\u3000', '')\n",
    "    main_text = re.sub('([！。])', r'\\1\\n', main_text)  # 。と！で改行\n",
    "    text_list = main_text.splitlines()\n",
    "    return text_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Wordcloudを用いた形態素解析"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WordCloudとは\n",
    "![](./utils/word_cloud.png)\n",
    "\n",
    "ニュースでこんなグラフをみたことはありませんか？\n",
    "\n",
    "WordCloudは、文章中で出現頻度が高い単語を複数選び出し、その頻度に応じた大きさで図示する手法。ウェブページやブログなどに頻出する単語を自動的に並べることなどを指す。文字の大きさだけでなく、色、字体、向きに変化をつけることで、文章の内容をひと目で印象づけることができる。(デジタル大辞泉の解説より)\n",
    "\n",
    "wordcloudで文字の出現頻度を可視化する方法を実装してみます。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "wordcloudはデフォルトでは日本語に対応していないので、適当な日本語フォントを読み込ませる必要があります。\n",
    "また、Google Colab には日本語の True Font が入っていないので、インストールします。このコマンドをコピペして実行します。\n",
    "!apt-get -y install fonts-ipafont-gothic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_list = get_url_text(url=\"https://www.aozora.gr.jp/cards/000148/files/773_14560.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from janome.tokenizer import Tokenizer\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Janome で単語単位に分割し、必要な単語だけ抜き出し、Word Cloud を実行します。\n",
    "単語単位に分割し、名詞（但し、非自立・代名詞・数を除く）のみを対象に抜き出し、words_wakati （単語が空白で区切られ改行コードはない状態）に保存します"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = Tokenizer()\n",
    "tokens = t.tokenize(\"\".join(text_list))\n",
    "word_list=[]\n",
    "for token in tokens:\n",
    "    word = token.surface\n",
    "    partOfSpeech = token.part_of_speech.split(',')[0]\n",
    "    partOfSpeech2 = token.part_of_speech.split(',')[1]\n",
    "     \n",
    "    word_list.append(word)\n",
    " \n",
    "words_wakati=\" \".join(word_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "コードをみて頂ければ分かると思います。参考に、# default 値を記載してあります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpath = './utils/GenShinGothic-Bold.ttf'  # 日本語フォント指\n",
    "wordcloud = WordCloud(\n",
    "    font_path=fpath,\n",
    "    width=900, height=600,   # default width=400, height=200\n",
    "    background_color=\"white\",   # default=”black”\n",
    "    max_words=500,   # default=200\n",
    "    min_font_size=4,   #default=4\n",
    "    collocations = False   #default = True\n",
    "    ).generate(words_wakati)\n",
    "plt.figure(figsize=(15,12))\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 表示させたくない単語の削除\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上記のワードクラウドを見ると、「する」「いる」のような、「そりゃ沢山使われるよね」という単語がデカデカと表示されています。\n",
    "そういった、「消したい単語」は、単語を連結する際に省いておく必要があります。\n",
    "\n",
    "具体的には、以下のように、単語が指定した語句一覧に含まれていたら文字列連結をやめる処理にすればOKです。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_list=[]\n",
    "for token in tokens:\n",
    "    word = token.surface\n",
    "    partOfSpeech = token.part_of_speech.split(',')[0]\n",
    "    partOfSpeech2 = token.part_of_speech.split(',')[1]\n",
    "    if word not in [\"それ\", \"ない\", \"する\", \"いる\"]:\n",
    "        word_list.append(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "また特定の品詞のみを使用することもできます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_list=[]\n",
    "for token in tokens:\n",
    "    word = token.surface\n",
    "    partOfSpeech = token.part_of_speech.split(',')[0]\n",
    "    partOfSpeech2 = token.part_of_speech.split(',')[1]\n",
    "    if partOfSpeech == \"名詞\":\n",
    "       if (partOfSpeech2 != \"非自立\") and (partOfSpeech2 != \"代名詞\") and (partOfSpeech2 != \"数\"):\n",
    "            word_list.append(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4 WordCloud作成+画像でマスク処理\n",
    "mask画像を使用すればニュースで見るようなWord Cloudを作成することも可能です。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n",
    "from wordcloud import ImageColorGenerator\n",
    "def get_wordcrowd_color_mask( text, img_color):\n",
    "    \n",
    "    wc = WordCloud(width=900,\n",
    "                   height=600,\n",
    "                   font_path=fpath,\n",
    "                   background_color=\"white\",\n",
    "                   mask=img_color,\n",
    "                   collocations=False, # 単語の重複しないように\n",
    "                  ).generate( text )\n",
    "\n",
    "    image_colors = ImageColorGenerator(img_color)\n",
    "\n",
    "    # show\n",
    "    plt.figure(figsize=(12,12))\n",
    "    plt.imshow(wc, # 元画像の色を使う\n",
    "               interpolation=\"bilinear\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "img_color = np.array(Image.open( './utils/mask1.png' ))\n",
    "get_wordcrowd_color_mask(words_wakati, img_color)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image, ImageOps\n",
    "# 画像を読み込んだ後、白黒反転した画像を作成\n",
    "img_color = Image.open('./utils/mask.jpg')\n",
    "im_invert = ImageOps.invert(img_color)\n",
    "im_invert = np.array(im_invert)\n",
    "get_wordcrowd_color_mask(words_wakati, im_invert)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 自然言語処理における前処理の種類\n",
    "以下スライド参照<br>\n",
    "![](./utils/自然言語処理/Slide35.jpg)\n",
    "![](./utils/自然言語処理/Slide36.jpg)\n",
    "![](./utils/自然言語処理/Slide37.jpg)\n",
    "![](./utils/自然言語処理/Slide38.jpg)\n",
    "![](./utils/自然言語処理/Slide39.jpg)\n",
    "![](./utils/自然言語処理/Slide40.jpg)\n",
    "![](./utils/自然言語処理/Slide45.jpg)\n",
    "![](./utils/自然言語処理/Slide46.jpg)\n",
    "![](./utils/自然言語処理/Slide47.jpg)\n",
    "![](./utils/自然言語処理/Slide48.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 単語をベクトル化とは？\n",
    "そもそもベクトルとは「大きさだけでなく、向きももった量」のことです。\n",
    "単語を「大きさだけでなく、向きももった量」にすると言う意味がよくわからないと思いますが、実際にやっていく実行していくなかで理解してもらいたいと思います。\n",
    "### 単語ベクトルとは\n",
    "単語ベクトルとは，単語を数百次元の実数値ベクトルとして表現したものであり，単語間の意味的な関係の強さをそれぞれの単語に対応するベクトルの成す角のコサインやユークリッド距離などを用いて定量化することを目的としています。<br>\n",
    "要はコンピュータで計算できないもの(単語)を計算できるようにすることを目的にしています。<br>\n",
    "また、単語ベクトルは単語の分散表現(Distributed Representation，Word Embedding）とも呼ばれ単語に留まらず文を表現させる研究も盛んに行われています。\n",
    "さまざまな単語ベクトル構成手法の内，昨今，注目されているのがニューラルネットワークに基づく学習手法です。<br>\n",
    "特に，Google の Mikolov らが提案した手法とそれを実装したツール word2vec2 は，大規模なコーパスで学習させることでベクトル空間内でking − man + woman を計算すると queen に近い値を出すといいたものです。\n",
    "今回はこれを皆さんに体験してもらおうと思います。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 単語の分散表現とは？\n",
    "単語の分散表現のメリットを理解するための比較対象として、単語の one-hot表現 についても説明します。\n",
    "#### one-hot表現\n",
    "文字や単語を、記号の世界から数値の世界に変換する手法として用いられるのが、「One-hotベクトル表現」です。One-hotベクトルとは、ベクトルのすべての要素のうちひとつだけが1であり、残りはすべて0であるベクトルを意味します。One-hotベクトル表現の言語処理への応用では、世の中に存在するすべての単語を、（0,0……,0,1,0,……0,0）のベクトルで表現します。これにより、単語とベクトルが一対一の関係になり、プログラムで処理できるようになります。このように、各概念を一対一対応で表現する手法は「局所表現」と呼ばれます。<br>\n",
    "しかしこの手法では、単語同士が関連していないため、同一単語であるかどうかの判定以外の処理を行うことは難しく、また世界に存在する数多くの言語のすべての単語を網羅するとなると、ベクトル数が膨大になり、計算時間が激増するという問題点がありました。<br>\n",
    "![](./utils/自然言語処理/Slide50.jpg)\n",
    "![](./utils/自然言語処理/Slide51.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 分散表現\n",
    "局所表現による言語処理の計算時間の膨大さという問題点を解決したのが、2000年に提案された、「分散表現」の言語処理への応用である単語分散表現の登場です。\n",
    "\n",
    "単語分散表現とは、「文字・単語をベクトル空間に埋め込み、その空間上のひとつの点として捉える」ことを指します。単語分散表現は、単語埋め込み（Word Embedding）とも呼ばれます。\n",
    "\n",
    "局所表現では、ある概念をほかの概念から完全に独立したものとして表現しています。一方、分散表現では、ある概念を表現する際に、ほかの概念との共通点や類似性と紐づけながら、ベクトル空間上に表現します。以下の例のベクトルの各要素は我々が理解できる食べ物に関連した概念ですが、ベクトルの各要素はアルゴリズムが勝手に作り出すものであり、人間に解釈できるものとは限りません。\n",
    "更に、それぞれの概念同士が関連し合っている故に、分散表現では異なる概念を表現するベクトル同士での計算が可能です。\n",
    "この分散表現は、人間が新しいことを記憶する際に、既に知っていることと関連させて記憶することや、あらゆる事象はさまざまな特徴で表現されうるといった、認知心理学や神経科学が人間の脳に関して持つ知見を応用し生まれたものです。\n",
    "\n",
    "![](./utils/自然言語処理/Slide52.jpg)\n",
    "![](./utils/自然言語処理/Slide53.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### なぜ自然言語処理にとって単語の分散表現は重要なのか？\n",
    "![](./utils/自然言語処理/Slide54.jpg)\n",
    "![](./utils/自然言語処理/Slide55.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 単語のベクトル化 word2vec\n",
    "![](./utils/自然言語処理/Slide56.jpg)\n",
    "![](./utils/自然言語処理/Slide57.jpg)\n",
    "![](./utils/自然言語処理/Slide58.jpg)\n",
    "![](./utils/自然言語処理/Slide59.jpg)\n",
    "![](./utils/自然言語処理/Slide60.jpg)\n",
    "![](./utils/自然言語処理/Slide61.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-hot表現を作ってみる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import janome.tokenizer #形態素解析器(日本語を単語に分割するライブラリ)\n",
    "\n",
    "# Documents\n",
    "d_01 = \"私は今朝おにぎりを食べました。\"\n",
    "d_02 = \"今日の朝はあめでした。犬の散歩には行きませんでした。\"\n",
    "d_03 = \"論文を読むのは楽しい。\"\n",
    "d_04 = \"あめは美味しい。\"\n",
    "\n",
    "# 分かち書き(Tokenを見出し語に戻す)\n",
    "tokenizer = janome.tokenizer.Tokenizer()\n",
    "print([token.base_form for token in tokenizer.tokenize(d_01)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vocabularyを作る\n",
    "tokens_01 = [token.base_form for token in tokenizer.tokenize(d_01)]\n",
    "tokens_02 = [token.base_form for token in tokenizer.tokenize(d_02)]\n",
    "tokens_03 = [token.base_form for token in tokenizer.tokenize(d_03)]\n",
    "tokens_04 = [token.base_form for token in tokenizer.tokenize(d_04)]\n",
    "vocabulary = list(set(tokens_01+tokens_02+tokens_03+tokens_04))\n",
    "print(vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 単語IDを表示\n",
    "for i in range(len(vocabulary)):\n",
    "    print(\"token ID : {}, token : {}\".format(i,vocabulary[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot化\n",
    "import sklearn.preprocessing\n",
    "vocabulary_onehot = sklearn.preprocessing.label_binarize(vocabulary,classes=vocabulary)\n",
    "\n",
    "for token, onehotvec in zip(vocabulary,vocabulary_onehot):\n",
    "    print(\"one-hot vector : {}, token : {}\".format(onehotvec,token))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 任意のOne-hot表現を取り出す\n",
    "token_index = vocabulary.index(\"私\")\n",
    "print(\"「私」のOne-hot表現は {}\".format(vocabulary_onehot[token_index]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "今回は分割と見出し語化(「食べ」→「食べる」等)のみ行いました。<br>\n",
    "実際には以下の様な処理が入ることが有ります。\n",
    "- クリーニング(htmlタグ除去等)\n",
    "- 正規化(半角全角、大文字小文字、表記ゆれの統一等)\n",
    "- ストップワード除去(頻出で意味のない単語。the等)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One-hot表現の問題点① : 次元が大きすぎる\n",
    "\n",
    "- 上記の例ではCorpusが小さかったので、Vocabularyの大きさも20程度でした。\n",
    "- 実際の日本語wikipediaコーパスは150万語程度です。\n",
    "- ある単語が専有するメモリは莫大なものになります。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "# 150万次元のOne-hotベクトルを作ってみて、メモリを確認する\n",
    "memory_check_list = [0] * 1500000\n",
    "memory_check_list[0] = 1\n",
    "memory = sys.getsizeof(memory_check_list)\n",
    "memory = memory / (1000*1000)\n",
    "print(\"ある単語が専有するメモリ : {:0.1f}MB\".format(memory))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One-hot表現の問題点② : 意味をエンコードできない\n",
    "\n",
    "- ベクトルは内積やコサイン類似度といった手法で「近さ」を定量化出来ます。\n",
    "- しかしOne-hot表現では、異なる単語ベクトルの内積・コサイン類似度は0になってしまいます。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inu_vec = vocabulary_onehot[vocabulary.index(\"犬\")]\n",
    "kesa_vec = vocabulary_onehot[vocabulary.index(\"今朝\")]\n",
    "asa_vec = vocabulary_onehot[vocabulary.index(\"朝\")]\n",
    "\n",
    "# 「犬」と「朝」の内積\n",
    "print(\"「犬」と「朝」の距離 : {}\".format(np.dot(inu_vec,asa_vec)))\n",
    "\n",
    "# 「今朝」と「朝」の内積\n",
    "print(\"「今朝」と「朝」の距離 : {}\".format(np.dot(kesa_vec,asa_vec)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "もし意味を表現できるようになれば、以下の事ができます。\n",
    "\n",
    "- 意味の近い単語ほど、ベクトルは近くなる(cos類似度が1に近くなる)。\n",
    "- 単語の足し引きで他の単語を表現できる。\n",
    "    -「パリ」ー「フランス」＋「ドイツ」≒「ベルリン」 (首都の意味をエンコード)\n",
    "    - 「おじさん」ー「男」＋「女」≒「おばさん」 (性別の意味をエンコード)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### word2vec\n",
    "\n",
    "-「次元が大きすぎる」「意味をエンコードできない」というOne-hot表現の弱点を克服する手法がword2vecです。\n",
    "- word2vecはCBOWとSkip-gramという2種類に分類出来ます。\n",
    "\n",
    "|分類 |\t意味|\n",
    "|----|----|\n",
    "|CBOW |\t周辺の単語から、ある単語を予測する|\n",
    "|Skip-gram| \tある単語から、周辺の単語を予測する|\n",
    "\n",
    "\n",
    "「周辺」のサイズはウィンドウサイズと呼び、ハイパーパラメータです。<br>\n",
    "「ある場所に入る単語の確率分布は、その周辺の単語によって決定される」という分布仮説という考え方があります。<br>\n",
    "だから「周辺の単語」から「ある単語」を予測する事ができます。(CBOW)<br>\n",
    "逆も然りです。(Skip-gram)<br>\n",
    "\n",
    "-   「ある単語」のベクトル表現を、150万次元から300次元に削減しています。\n",
    "-   この「300次元のベクトル」はスパース(One-hotの様に、多くが0の意味)ではなく、密なベクトルです。\n",
    "-    このやり方で得られたベクトルは、意味をエンコードできている事が報告されています。<br>\n",
    "それでは実行してみましょう。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "分かち書きには`janome`を使用します。\n",
    "#### Analyzerモジュール\n",
    "Analyzerの主な前処理と後処理は以下の通りです。\n",
    "\n",
    "前処理\n",
    "文章の抽出、不要な文字列の削除(HTMLタグなど)、文字種の統一(英字は全て英小文字にするなど)、スペルミス・変換ミスなどによる表記ゆらぎの補正などをします。\n",
    "\n",
    "後処理\n",
    "分かち書き後の字句(トークン)を対象としており、数字の置換(数字の名詞は全て0に置き換えるなど)、特定の品詞のみの抽出をします。\n",
    "\n",
    "Analyzerは、以下の3つを組み合わせて使用します。\n",
    "\n",
    "- 文字の正規化などの前処理を行うCharFilter\n",
    "- 形態素解析後の後処理を行うTokenFilter\n",
    "- 分かち書きされたトークン単位で処理するTokenFilter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CharFilter\n",
    "| UnicodeNormalizeCharFilter | Unicodeを正規化することで表記ゆれを吸収する<br> - 引数に’NFC’, ‘NFKC’, ‘NFD’, ‘NFKD’を指定可能<br> - デフォルトは’NFKC’で、全角→半角などの変換が行われる|\n",
    "| -----|------|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved at \"kokoro.model\"\n"
     ]
    }
   ],
   "source": [
    "from janome.analyzer import Analyzer\n",
    "from janome.tokenfilter import POSKeepFilter\n",
    "from gensim.models import word2vec\n",
    "\n",
    "def save_model():\n",
    "    token_filters = [POSKeepFilter(['名詞', '代名詞'])]\n",
    "    a = Analyzer(token_filters=token_filters)\n",
    "    sentences = [[tok.surface for tok in a.analyze(main_text)]]\n",
    "    model = word2vec.Word2Vec(sentences, min_count=1, size=150)\n",
    "    model.save('model/kokoro.model')\n",
    "    print('saved at \"kokoro.model\"')\n",
    "save_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'桜': <gensim.models.keyedvectors.Vocab at 0x7f8ad40e5160>,\n",
       " '樹': <gensim.models.keyedvectors.Vocab at 0x7f8ad40e5128>,\n",
       " '下': <gensim.models.keyedvectors.Vocab at 0x7f8ad40e5eb8>,\n",
       " '屍体': <gensim.models.keyedvectors.Vocab at 0x7f8ad40e5f28>,\n",
       " 'これ': <gensim.models.keyedvectors.Vocab at 0x7f8ad40e5fd0>,\n",
       " 'こと': <gensim.models.keyedvectors.Vocab at 0x7f8ad40e5f60>,\n",
       " 'ん': <gensim.models.keyedvectors.Vocab at 0x7f8ad40e5f98>,\n",
       " '花': <gensim.models.keyedvectors.Vocab at 0x7f8ad40e5ef0>,\n",
       " '見事': <gensim.models.keyedvectors.Vocab at 0x7f8aeb6ede48>,\n",
       " '俺': <gensim.models.keyedvectors.Vocab at 0x7f8aeb6edcf8>,\n",
       " 'さ': <gensim.models.keyedvectors.Vocab at 0x7f8aeb6eddd8>,\n",
       " '二': <gensim.models.keyedvectors.Vocab at 0x7f8aeb6edd68>,\n",
       " '三': <gensim.models.keyedvectors.Vocab at 0x7f8aeb6ed9b0>,\n",
       " '日': <gensim.models.keyedvectors.Vocab at 0x7f8aeb6edd30>,\n",
       " '不安': <gensim.models.keyedvectors.Vocab at 0x7f8aeb6ed1d0>,\n",
       " 'いま': <gensim.models.keyedvectors.Vocab at 0x7f8aeb6ed198>,\n",
       " 'とき': <gensim.models.keyedvectors.Vocab at 0x7f8ad404bc18>,\n",
       " '毎晩': <gensim.models.keyedvectors.Vocab at 0x7f8ad404bc50>,\n",
       " '家': <gensim.models.keyedvectors.Vocab at 0x7f8ad4199940>,\n",
       " '道': <gensim.models.keyedvectors.Vocab at 0x7f8ad41997f0>,\n",
       " '部屋': <gensim.models.keyedvectors.Vocab at 0x7f8ad4199ef0>,\n",
       " '数': <gensim.models.keyedvectors.Vocab at 0x7f8ad4199390>,\n",
       " '道具': <gensim.models.keyedvectors.Vocab at 0x7f8ad4199a58>,\n",
       " 'うち': <gensim.models.keyedvectors.Vocab at 0x7f8ad4199f28>,\n",
       " 'ちっぽけ': <gensim.models.keyedvectors.Vocab at 0x7f8ad4199978>,\n",
       " '薄っぺら': <gensim.models.keyedvectors.Vocab at 0x7f8ad4199fd0>,\n",
       " 'いも': <gensim.models.keyedvectors.Vocab at 0x7f8ad4199e80>,\n",
       " '安全': <gensim.models.keyedvectors.Vocab at 0x7f8ad4199dd8>,\n",
       " '剃刀': <gensim.models.keyedvectors.Vocab at 0x7f8ad4199e10>,\n",
       " '刃': <gensim.models.keyedvectors.Vocab at 0x7f8ad4199f60>,\n",
       " '千里眼': <gensim.models.keyedvectors.Vocab at 0x7f8ad4199da0>,\n",
       " 'よう': <gensim.models.keyedvectors.Vocab at 0x7f8ad4199d30>,\n",
       " 'の': <gensim.models.keyedvectors.Vocab at 0x7f8ad4199d68>,\n",
       " 'おまえ': <gensim.models.keyedvectors.Vocab at 0x7f8ad4199cf8>,\n",
       " 'それ': <gensim.models.keyedvectors.Vocab at 0x7f8ad4199c88>,\n",
       " 'ちがい': <gensim.models.keyedvectors.Vocab at 0x7f8ad4199cc0>,\n",
       " '真っ盛り': <gensim.models.keyedvectors.Vocab at 0x7f8ad4199c50>,\n",
       " '状態': <gensim.models.keyedvectors.Vocab at 0x7f8ad4199860>,\n",
       " 'あたり': <gensim.models.keyedvectors.Vocab at 0x7f8ad4199278>,\n",
       " '空気': <gensim.models.keyedvectors.Vocab at 0x7f8ad4199320>,\n",
       " 'なか': <gensim.models.keyedvectors.Vocab at 0x7f8ad4199b70>,\n",
       " '一種': <gensim.models.keyedvectors.Vocab at 0x7f8ad41999b0>,\n",
       " '神秘': <gensim.models.keyedvectors.Vocab at 0x7f8ad41993c8>,\n",
       " '雰囲気': <gensim.models.keyedvectors.Vocab at 0x7f8ad4199438>,\n",
       " 'もの': <gensim.models.keyedvectors.Vocab at 0x7f8ad41992b0>,\n",
       " '独楽': <gensim.models.keyedvectors.Vocab at 0x7f8ad4199400>,\n",
       " '完全': <gensim.models.keyedvectors.Vocab at 0x7f8ad41998d0>,\n",
       " '静止': <gensim.models.keyedvectors.Vocab at 0x7f8ad4199b00>,\n",
       " '音楽': <gensim.models.keyedvectors.Vocab at 0x7f8aeb75a470>,\n",
       " '上手': <gensim.models.keyedvectors.Vocab at 0x7f8aeb75a860>,\n",
       " '演奏': <gensim.models.keyedvectors.Vocab at 0x7f8aeb75a400>,\n",
       " 'なに': <gensim.models.keyedvectors.Vocab at 0x7f8aeb75a6d8>,\n",
       " '幻覚': <gensim.models.keyedvectors.Vocab at 0x7f8aeb75a828>,\n",
       " '灼熱': <gensim.models.keyedvectors.Vocab at 0x7f8aeb75a9e8>,\n",
       " '生殖': <gensim.models.keyedvectors.Vocab at 0x7f8aeb75a940>,\n",
       " '後光': <gensim.models.keyedvectors.Vocab at 0x7f8aeb75a710>,\n",
       " '人': <gensim.models.keyedvectors.Vocab at 0x7f8aeb75a1d0>,\n",
       " '心': <gensim.models.keyedvectors.Vocab at 0x7f8aeb75a7f0>,\n",
       " '不思議': <gensim.models.keyedvectors.Vocab at 0x7f8aeb75a668>,\n",
       " '昨日': <gensim.models.keyedvectors.Vocab at 0x7f8aeb75a320>,\n",
       " '一昨日': <gensim.models.keyedvectors.Vocab at 0x7f8aeb75a550>,\n",
       " '陰気': <gensim.models.keyedvectors.Vocab at 0x7f8aeb75a438>,\n",
       " '気': <gensim.models.keyedvectors.Vocab at 0x7f8aeb75ac50>,\n",
       " '反対': <gensim.models.keyedvectors.Vocab at 0x7f8aeb75a630>,\n",
       " '憂鬱': <gensim.models.keyedvectors.Vocab at 0x7f8ae61024a8>,\n",
       " '空虚': <gensim.models.keyedvectors.Vocab at 0x7f8ae6102470>,\n",
       " '気持': <gensim.models.keyedvectors.Vocab at 0x7f8ae6102438>,\n",
       " '爛漫': <gensim.models.keyedvectors.Vocab at 0x7f8ae6102400>,\n",
       " '一つ': <gensim.models.keyedvectors.Vocab at 0x7f8ae61023c8>,\n",
       " '想像': <gensim.models.keyedvectors.Vocab at 0x7f8ae6102390>,\n",
       " '何': <gensim.models.keyedvectors.Vocab at 0x7f8ae6102358>,\n",
       " '納得': <gensim.models.keyedvectors.Vocab at 0x7f8ae6102320>,\n",
       " '馬': <gensim.models.keyedvectors.Vocab at 0x7f8ae61022e8>,\n",
       " '犬': <gensim.models.keyedvectors.Vocab at 0x7f8ae61022b0>,\n",
       " '猫': <gensim.models.keyedvectors.Vocab at 0x7f8ae6102278>,\n",
       " '人間': <gensim.models.keyedvectors.Vocab at 0x7f8ae6102240>,\n",
       " 'みな': <gensim.models.keyedvectors.Vocab at 0x7f8ae6102208>,\n",
       " '腐爛': <gensim.models.keyedvectors.Vocab at 0x7f8ae61021d0>,\n",
       " '蛆': <gensim.models.keyedvectors.Vocab at 0x7f8ae6102198>,\n",
       " '水晶': <gensim.models.keyedvectors.Vocab at 0x7f8ae6102160>,\n",
       " '液': <gensim.models.keyedvectors.Vocab at 0x7f8ae6102128>,\n",
       " '根': <gensim.models.keyedvectors.Vocab at 0x7f8ae61020f0>,\n",
       " '貪婪': <gensim.models.keyedvectors.Vocab at 0x7f8ae61020b8>,\n",
       " '蛸': <gensim.models.keyedvectors.Vocab at 0x7f8ae6102080>,\n",
       " '食': <gensim.models.keyedvectors.Vocab at 0x7f8ae6102048>,\n",
       " '糸': <gensim.models.keyedvectors.Vocab at 0x7f8ae6102668>,\n",
       " '毛根': <gensim.models.keyedvectors.Vocab at 0x7f8ae61025c0>,\n",
       " '聚': <gensim.models.keyedvectors.Vocab at 0x7f8ae6102550>,\n",
       " 'め': <gensim.models.keyedvectors.Vocab at 0x7f8ae61025f8>,\n",
       " '液体': <gensim.models.keyedvectors.Vocab at 0x7f8ae6102630>,\n",
       " '花弁': <gensim.models.keyedvectors.Vocab at 0x7f8ae6102518>,\n",
       " '蕊': <gensim.models.keyedvectors.Vocab at 0x7f8ae6102898>,\n",
       " '静か': <gensim.models.keyedvectors.Vocab at 0x7f8ae61028d0>,\n",
       " '行列': <gensim.models.keyedvectors.Vocab at 0x7f8ae6102908>,\n",
       " '維管束': <gensim.models.keyedvectors.Vocab at 0x7f8ae61026a0>,\n",
       " '夢': <gensim.models.keyedvectors.Vocab at 0x7f8ae61026d8>,\n",
       " 'そう': <gensim.models.keyedvectors.Vocab at 0x7f8ae6102710>,\n",
       " '顔': <gensim.models.keyedvectors.Vocab at 0x7f8ae6102748>,\n",
       " '透視': <gensim.models.keyedvectors.Vocab at 0x7f8ae6102780>,\n",
       " '術': <gensim.models.keyedvectors.Vocab at 0x7f8ae61027b8>,\n",
       " '瞳': <gensim.models.keyedvectors.Vocab at 0x7f8ae61027f0>,\n",
       " '自由': <gensim.models.keyedvectors.Vocab at 0x7f8ae6102828>,\n",
       " '前': <gensim.models.keyedvectors.Vocab at 0x7f8ae6102860>,\n",
       " 'ここ': <gensim.models.keyedvectors.Vocab at 0x7f8ae6102940>,\n",
       " '溪': <gensim.models.keyedvectors.Vocab at 0x7f8ae6102978>,\n",
       " '石': <gensim.models.keyedvectors.Vocab at 0x7f8ae61029b0>,\n",
       " '上': <gensim.models.keyedvectors.Vocab at 0x7f8ae61029e8>,\n",
       " '歩き': <gensim.models.keyedvectors.Vocab at 0x7f8ae6102a20>,\n",
       " '水': <gensim.models.keyedvectors.Vocab at 0x7f8ae6102a58>,\n",
       " 'しぶき': <gensim.models.keyedvectors.Vocab at 0x7f8ae6102a90>,\n",
       " 'あちら': <gensim.models.keyedvectors.Vocab at 0x7f8ae6102ac8>,\n",
       " 'こちら': <gensim.models.keyedvectors.Vocab at 0x7f8ae6102b00>,\n",
       " '羽': <gensim.models.keyedvectors.Vocab at 0x7f8ae6102b38>,\n",
       " 'かげろう': <gensim.models.keyedvectors.Vocab at 0x7f8ae6102b70>,\n",
       " 'アフロディット': <gensim.models.keyedvectors.Vocab at 0x7f8ae6102ba8>,\n",
       " '空': <gensim.models.keyedvectors.Vocab at 0x7f8ae6102be0>,\n",
       " 'とおり': <gensim.models.keyedvectors.Vocab at 0x7f8ae6102c18>,\n",
       " '彼ら': <gensim.models.keyedvectors.Vocab at 0x7f8ae6102c50>,\n",
       " 'そこ': <gensim.models.keyedvectors.Vocab at 0x7f8ae6102c88>,\n",
       " '結婚': <gensim.models.keyedvectors.Vocab at 0x7f8ae6102cc0>,\n",
       " '変': <gensim.models.keyedvectors.Vocab at 0x7f8ae6102cf8>,\n",
       " '磧': <gensim.models.keyedvectors.Vocab at 0x7f8ae6102d30>,\n",
       " '水溜': <gensim.models.keyedvectors.Vocab at 0x7f8ae6102d68>,\n",
       " '石油': <gensim.models.keyedvectors.Vocab at 0x7f8ae6102da0>,\n",
       " '光彩': <gensim.models.keyedvectors.Vocab at 0x7f8ae6102dd8>,\n",
       " '一': <gensim.models.keyedvectors.Vocab at 0x7f8ae6102e10>,\n",
       " '面': <gensim.models.keyedvectors.Vocab at 0x7f8ae6102e48>,\n",
       " '万': <gensim.models.keyedvectors.Vocab at 0x7f8ae6102e80>,\n",
       " '匹': <gensim.models.keyedvectors.Vocab at 0x7f8ae6102eb8>,\n",
       " '隙間': <gensim.models.keyedvectors.Vocab at 0x7f8ae6102ef0>,\n",
       " '翅': <gensim.models.keyedvectors.Vocab at 0x7f8ae6102f28>,\n",
       " '光': <gensim.models.keyedvectors.Vocab at 0x7f8ae6102f60>,\n",
       " '油': <gensim.models.keyedvectors.Vocab at 0x7f8ae6102f98>,\n",
       " '産卵': <gensim.models.keyedvectors.Vocab at 0x7f8ae6102fd0>,\n",
       " '墓場': <gensim.models.keyedvectors.Vocab at 0x7f8ae623b048>,\n",
       " '胸': <gensim.models.keyedvectors.Vocab at 0x7f8ae623b080>,\n",
       " '変質': <gensim.models.keyedvectors.Vocab at 0x7f8ae623b0b8>,\n",
       " '者': <gensim.models.keyedvectors.Vocab at 0x7f8ae623b0f0>,\n",
       " '残忍': <gensim.models.keyedvectors.Vocab at 0x7f8ae623b128>,\n",
       " '溪間': <gensim.models.keyedvectors.Vocab at 0x7f8ae623b160>,\n",
       " '鶯': <gensim.models.keyedvectors.Vocab at 0x7f8ae623b198>,\n",
       " '四十雀': <gensim.models.keyedvectors.Vocab at 0x7f8ae623b1d0>,\n",
       " '日光': <gensim.models.keyedvectors.Vocab at 0x7f8ae623b208>,\n",
       " '青': <gensim.models.keyedvectors.Vocab at 0x7f8ae623b240>,\n",
       " '木': <gensim.models.keyedvectors.Vocab at 0x7f8ae623b278>,\n",
       " '若芽': <gensim.models.keyedvectors.Vocab at 0x7f8ae623b2b0>,\n",
       " 'もうろう': <gensim.models.keyedvectors.Vocab at 0x7f8ae623b2e8>,\n",
       " '心象': <gensim.models.keyedvectors.Vocab at 0x7f8ae623b320>,\n",
       " '惨劇': <gensim.models.keyedvectors.Vocab at 0x7f8ae623b358>,\n",
       " '必要': <gensim.models.keyedvectors.Vocab at 0x7f8ae623b390>,\n",
       " '平衡': <gensim.models.keyedvectors.Vocab at 0x7f8ae623b3c8>,\n",
       " '明確': <gensim.models.keyedvectors.Vocab at 0x7f8ae623b400>,\n",
       " '悪鬼': <gensim.models.keyedvectors.Vocab at 0x7f8ae623b438>,\n",
       " '完成': <gensim.models.keyedvectors.Vocab at 0x7f8ae623b470>,\n",
       " '腋の下': <gensim.models.keyedvectors.Vocab at 0x7f8ae623b4a8>,\n",
       " '冷汗': <gensim.models.keyedvectors.Vocab at 0x7f8ae623b4e0>,\n",
       " '不愉快': <gensim.models.keyedvectors.Vocab at 0x7f8ae623b518>,\n",
       " '精液': <gensim.models.keyedvectors.Vocab at 0x7f8ae623b550>,\n",
       " 'ごらん': <gensim.models.keyedvectors.Vocab at 0x7f8ae623b588>,\n",
       " '達': <gensim.models.keyedvectors.Vocab at 0x7f8ae623b5c0>,\n",
       " 'どこ': <gensim.models.keyedvectors.Vocab at 0x7f8ae623b5f8>,\n",
       " '空想': <gensim.models.keyedvectors.Vocab at 0x7f8ae623b630>,\n",
       " '見当': <gensim.models.keyedvectors.Vocab at 0x7f8ae623b668>,\n",
       " '頭': <gensim.models.keyedvectors.Vocab at 0x7f8ae623b6a0>,\n",
       " '今': <gensim.models.keyedvectors.Vocab at 0x7f8ae623b6d8>,\n",
       " '酒宴': <gensim.models.keyedvectors.Vocab at 0x7f8ae623b710>,\n",
       " '村人': <gensim.models.keyedvectors.Vocab at 0x7f8ae623b748>,\n",
       " 'たち': <gensim.models.keyedvectors.Vocab at 0x7f8ae623b780>,\n",
       " '権利': <gensim.models.keyedvectors.Vocab at 0x7f8ae623b7b8>,\n",
       " '花見': <gensim.models.keyedvectors.Vocab at 0x7f8ae623b7f0>,\n",
       " '酒': <gensim.models.keyedvectors.Vocab at 0x7f8ae623b828>,\n",
       " '恥': <gensim.models.keyedvectors.Vocab at 0x7f8ae623b860>}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "あちら 0.01\n",
      "俺 0.01\n",
      "産卵 0.01\n",
      "何 0.01\n",
      "これ 0.01\n",
      "それ 0.01\n",
      "変 0.01\n",
      "精液 0.01\n",
      "毛根 0.01\n"
     ]
    }
   ],
   "source": [
    "# ある単語と類似している単語を見る\n",
    "\n",
    "# 尾張と類似している単語を見る\n",
    "similar_words = model.wv.most_similar(positive=[\"権利\"], topn=9)\n",
    "print(*[\" \".join([v, str(\"{:.2f}\".format(s))]) for v, s in similar_words], sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = word2vec.Word2Vec.load('model/kokoro.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('結婚', 0.20780237019062042)\n",
      "('見当', 0.1819564402103424)\n",
      "('毛根', 0.17572791874408722)\n",
      "('冷汗', 0.17085781693458557)\n",
      "('そう', 0.16998356580734253)\n",
      "('匹', 0.16660360991954803)\n",
      "('彼ら', 0.16545023024082184)\n",
      "('心', 0.16394442319869995)\n",
      "('もうろう', 0.1458818018436432)\n",
      "('陰気', 0.1398269385099411)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "results = model.wv.most_similar(negative=['花', '恥'])\n",
    "for result in results:\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 分散表現・単語のリストを取得\n",
    "word_vectors = model.wv.vectors\n",
    "index2word = model.wv.index2word\n",
    "# 名詞のindexを取得\n",
    "nouns_id = [i for i, n in enumerate(index2word)]\n",
    "\n",
    "# 品詞が名詞である単語の上位500語を抽出\n",
    "word_vectors = word_vectors[nouns_id][:500]\n",
    "index2word = [index2word[i] for i in nouns_id][:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorboardX import SummaryWriter\n",
    "import torch\n",
    "writer = SummaryWriter('./runs')\n",
    "writer.add_embedding(torch.FloatTensor(word_vectors), metadata=index2word)\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorboardによる可視化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Serving TensorBoard on localhost; to expose to the network, use a proxy or pass --bind_all\n",
      "TensorBoard 2.2.2 at http://localhost:6007/ (Press CTRL+C to quit)\n",
      "W0324 12:13:46.023861 140683524433664 security_validator.py:51] In 3.0, this warning will become an error:\n",
      "X-Content-Type-Options is required to be \"nosniff\"\n",
      "W0324 12:13:46.055906 140683524433664 security_validator.py:51] In 3.0, this warning will become an error:\n",
      "X-Content-Type-Options is required to be \"nosniff\"\n",
      "W0324 12:13:46.056118 140683524433664 security_validator.py:51] In 3.0, this warning will become an error:\n",
      "Requires default-src for Content-Security-Policy\n",
      "W0324 12:13:46.225122 140683524433664 security_validator.py:51] In 3.0, this warning will become an error:\n",
      "X-Content-Type-Options is required to be \"nosniff\"\n",
      "W0324 12:13:47.866306 140683524433664 application.py:594] path /data/plugin/whatif/data/plugins_listing not found, sending 404\n",
      "W0324 12:13:48.025595 140683524433664 application.py:594] path /data/plugin/whatif/data/plugins_listing not found, sending 404\n",
      "W0324 12:13:48.031019 140683524433664 application.py:594] path /data/plugin/whatif/data/plugins_listing not found, sending 404\n",
      "W0324 12:13:48.127430 140683524433664 application.py:594] path /data/plugin/whatif/data/plugins_listing not found, sending 404\n",
      "^C\n"
     ]
    }
   ],
   "source": [
    "!tensorboard --logdir=./runs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b10430241691b249f3f5968ff1d5f373841e9f0f1c6e818b42478bb30bce80ad"
  },
  "kernelspec": {
   "display_name": "Python 3.6.6 ('docker-webapi')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
